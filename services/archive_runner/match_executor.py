"""
MODULE: services.archive_runner.match_executor
RESPONSIBILITY: Execute multi-threaded search matching on files.
ALLOWED: MatchFinder, ThreadPoolExecutor, logging, json, time.
FORBIDDEN: Database connection creation (passed in).
ERRORS: None.

–ú–æ–¥—É–ª—å –¥–ª—è –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–π –≤ Excel —Ñ–∞–π–ª–∞—Ö.
"""

from __future__ import annotations

import time
from concurrent.futures import ThreadPoolExecutor, as_completed, TimeoutError as FutureTimeoutError
from pathlib import Path
from typing import Callable, Dict, List, Optional, Tuple

from loguru import logger

from services.error_logger import get_error_logger

from services.document_search.match_finder import MatchFinder


class MatchExecutor:
    """–ó–∞–ø—É—Å–∫–∞–µ—Ç –º–Ω–æ–≥–æ–ø–æ—Ç–æ—á–Ω—ã–π –ø–æ–∏—Å–∫ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–π –ø–æ —Å–ø–∏—Å–∫—É Excel —Ñ–∞–π–ª–æ–≤."""

    def __init__(
        self,
        base_match_finder: MatchFinder,
        max_workers: int = 2,
        get_avg_time_func: Optional[Callable[[], float]] = None,
        batch_delay: float = 5.0,
    ):
        self.base_match_finder = base_match_finder
        self.max_workers = max(1, max_workers)
        self._get_avg_time = get_avg_time_func
        self.batch_delay = max(0.0, batch_delay)

    def run(self, workbook_paths: List[Path]) -> Dict[str, Any]:
        """
        –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç —Ñ–∞–π–ª—ã –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è –∏ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –ø—Ä–æ–±–ª–µ–º–Ω—ã—Ö —Ñ–∞–π–ª–∞—Ö.
        
        Returns:
            Dict —Å –∫–ª—é—á–∞–º–∏:
            - "matches": List[Dict] - —Å–ø–∏—Å–æ–∫ –ª—É—á—à–∏—Ö —Å–æ–≤–ø–∞–¥–µ–Ω–∏–π –ø–æ –∫–∞–∂–¥–æ–º—É —Ç–æ–≤–∞—Ä—É
            - "failed_files": List[Dict] - —Å–ø–∏—Å–æ–∫ –ø—Ä–æ–±–ª–µ–º–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤ —Å –æ—à–∏–±–∫–∞–º–∏
                –ö–∞–∂–¥—ã–π —ç–ª–µ–º–µ–Ω—Ç: {"path": str, "error": str, "file_size_mb": float}
        """
        # #region agent log
        import json
        import os
        project_root = os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
        log_path = os.path.join(project_root, ".cursor", "debug.log")
        try:
            with open(log_path, "a", encoding="utf-8") as f:
                f.write(json.dumps({
                    "sessionId": "debug-session",
                    "runId": "doc-processing-debug",
                    "hypothesisId": "MATCH_EXECUTOR_START",
                    "location": "match_executor.py:run:start",
                    "message": "–ù–∞—á–∏–Ω–∞–µ–º match_executor.run",
                    "data": {
                        "workbook_paths_count": len(workbook_paths),
                        "sample_paths": [str(p)[-50:] for p in workbook_paths[:3]] if workbook_paths else []
                    },
                    "timestamp": int(time.time() * 1000)
                }))
        except Exception as e:
            pass
        # #endregion

        matches: Dict[str, Dict[str, Any]] = {}
        failed_files: List[Dict[str, Any]] = []  # –°–ø–∏—Å–æ–∫ –ø—Ä–æ–±–ª–µ–º–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤
        unique_paths = list({Path(p).resolve() for p in workbook_paths})
        total_files = len(unique_paths)
        duplicates_removed = len(workbook_paths) - total_files

        if total_files == 0:
            return {"matches": [], "failed_files": []}

        if duplicates_removed > 0:
            logger.warning(
                f"–û–±–Ω–∞—Ä—É–∂–µ–Ω–æ {duplicates_removed} –¥—É–±–ª–∏–∫–∞—Ç–æ–≤, –±—É–¥–µ—Ç –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ —Ñ–∞–π–ª–æ–≤: {total_files}"
            )
        else:
            logger.info(f"–ù–∞—á–∏–Ω–∞–µ–º –æ–±—Ä–∞–±–æ—Ç–∫—É {total_files} —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤")

        workers = min(self.max_workers, total_files)
        
        # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Ñ–∞–π–ª—ã –ø–∞—Ä—Ç–∏—è–º–∏ –¥–ª—è —Å–Ω–∏–∂–µ–Ω–∏—è –Ω–∞–≥—Ä—É–∑–∫–∏ –Ω–∞ CPU
        # –†–∞–∑–º–µ—Ä –ø–∞—Ä—Ç–∏–∏ = –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–æ—Ç–æ–∫–æ–≤ * 2, –Ω–æ –Ω–µ –±–æ–ª–µ–µ 10 —Ñ–∞–π–ª–æ–≤
        batch_size = min(workers * 2, 10, total_files)

        def process_file(workbook_path: Path) -> Tuple[Path, List[Dict[str, Any]], Optional[Exception], float]:
            start_time = time.time()
            logger.info(f"–ù–∞—á–∏–Ω–∞–µ–º –æ–±—Ä–∞–±–æ—Ç–∫—É —Ñ–∞–π–ª–∞: {workbook_path.name}")
            thread_match_finder = MatchFinder(
                self.base_match_finder.product_names,
                stop_phrases=getattr(self.base_match_finder, "stop_phrases", None),
                user_search_phrases=getattr(self.base_match_finder, "user_search_phrases", None),
            )
            try:
                # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ç–∏–ø —Ñ–∞–π–ª–∞
                suffix = workbook_path.suffix.lower()
                is_pdf = suffix == ".pdf"
                is_word = suffix in {".docx", ".doc"}
                
                if is_pdf:
                    # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º PDF —Ñ–∞–π–ª—ã
                    file_matches = thread_match_finder.search_pdf_for_products(workbook_path)
                    additional_matches = thread_match_finder.search_additional_phrases_in_pdf(workbook_path)
                elif is_word:
                    # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º Word –¥–æ–∫—É–º–µ–Ω—Ç—ã
                    file_matches = thread_match_finder.search_word_for_products(workbook_path)
                    additional_matches = thread_match_finder.search_additional_phrases_in_word(workbook_path)
                else:
                    # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º Excel —Ñ–∞–π–ª—ã
                    file_matches = thread_match_finder.search_workbook_for_products(workbook_path)
                    additional_matches = thread_match_finder.search_additional_phrases(workbook_path)
                
                # –û–±—ä–µ–¥–∏–Ω—è–µ–º –≤—Å–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è
                all_matches = file_matches + additional_matches
                
                elapsed = time.time() - start_time
                
                # –ü–æ–¥—Å—á–∏—Ç—ã–≤–∞–µ–º –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ —Ñ—Ä–∞–∑—ã –æ—Ç–¥–µ–ª—å–Ω–æ –¥–ª—è –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
                additional_count = sum(1 for m in additional_matches if m.get("is_additional_phrase"))
                
                logger.debug(
                    f"–§–∞–π–ª {workbook_path.name} –æ–±—Ä–∞–±–æ—Ç–∞–Ω –∑–∞ {elapsed:.1f} —Å–µ–∫: "
                    f"–æ—Å–Ω–æ–≤–Ω—ã—Ö —Å–æ–≤–ø–∞–¥–µ–Ω–∏–π {len(file_matches)}, "
                    f"–¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã—Ö —Ñ—Ä–∞–∑ {additional_count}, "
                    f"–≤—Å–µ–≥–æ {len(all_matches)}"
                )
                
                if additional_count > 0:
                    logger.info(
                        f"–ù–∞–π–¥–µ–Ω–æ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã—Ö —Ñ—Ä–∞–∑ –≤ —Ñ–∞–π–ª–µ {workbook_path.name}: {additional_count}"
                    )
                
                return workbook_path, all_matches, None, elapsed
            except Exception as error:
                elapsed = time.time() - start_time
                logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ —Ñ–∞–π–ª–∞ {workbook_path.name}: {error}")
                return workbook_path, [], error, elapsed

        avg_time_per_file = self._get_avg_time() if self._get_avg_time else 0.0
        processed_count = 0
        failed_count = 0
        total_elapsed_time = 0.0

        logger.info(f"–ó–∞–ø—É—Å–∫–∞–µ–º –æ–±—Ä–∞–±–æ—Ç–∫—É {total_files} —Ñ–∞–π–ª–æ–≤ –≤ {workers} –ø–æ—Ç–æ–∫–æ–≤ (–ø–∞—Ä—Ç–∏—è–º–∏ –ø–æ {batch_size} —Ñ–∞–π–ª–æ–≤)")
        
        # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Ñ–∞–π–ª—ã –ø–∞—Ä—Ç–∏—è–º–∏ –¥–ª—è —Å–Ω–∏–∂–µ–Ω–∏—è –Ω–∞–≥—Ä—É–∑–∫–∏ –Ω–∞ CPU
        batch_number = 0
        
        for batch_start in range(0, total_files, batch_size):
            batch_end = min(batch_start + batch_size, total_files)
            batch_paths = unique_paths[batch_start:batch_end]
            batch_number += 1
            
            logger.info(f"üì¶ –ü–∞—Ä—Ç–∏—è {batch_number}: –æ–±—Ä–∞–±–æ—Ç–∫–∞ —Ñ–∞–π–ª–æ–≤ {batch_start + 1}-{batch_end} –∏–∑ {total_files}")
            
            # #region agent log
            import json
            import os
            project_root = os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
            log_path = os.path.join(project_root, ".cursor", "debug.log")
            try:
                with open(log_path, "a", encoding="utf-8") as f:
                    f.write(json.dumps({
                        "sessionId": "debug-session",
                        "runId": "transaction-debug",
                        "hypothesisId": "MATCH_EXECUTOR_IDLE",
                        "location": "match_executor.py:run:thread_pool_start",
                        "message": "–ó–∞–ø—É—Å–∫ ThreadPoolExecutor –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ñ–∞–π–ª–æ–≤",
                        "data": {"workers": workers, "batch_size": len(batch_paths)},
                        "timestamp": int(__import__('time').time() * 1000)
                    }) + "\n")
            except Exception:
                pass
            # #endregion
            
            with ThreadPoolExecutor(max_workers=workers) as executor:
                future_to_path = {
                    executor.submit(process_file, workbook_path): workbook_path
                    for workbook_path in batch_paths
                }
                
                logger.debug(f"–ü–∞—Ä—Ç–∏—è {batch_number}: –≤—Å–µ –∑–∞–¥–∞—á–∏ –æ—Ç–ø—Ä–∞–≤–ª–µ–Ω—ã –≤ –æ—á–µ—Ä–µ–¥—å, –æ–∂–∏–¥–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã...")
            
                # –û—Ç—Å–ª–µ–∂–∏–≤–∞–µ–º –∑–∞–≤–∏—Å—à–∏–µ –∑–∞–¥–∞—á–∏
                last_progress_log = time.time()

                for future in as_completed(future_to_path):
                    # –õ–æ–≥–∏—Ä—É–µ–º, –µ—Å–ª–∏ –¥–æ–ª–≥–æ –∂–¥–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
                    current_time = time.time()
                    if current_time - last_progress_log > 30:
                        remaining_futures = len([f for f in future_to_path.keys() if not f.done()])
                        logger.warning(
                            f"‚è≥ –û–∂–∏–¥–∞–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤: –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ {processed_count}/{total_files}, "
                            f"–æ–∂–∏–¥–∞–µ–º {remaining_futures} —Ñ–∞–π–ª–æ–≤"
                        )
                        last_progress_log = current_time
                    
                    workbook_path = future_to_path[future]
                    processed_count += 1
                    try:
                        file_size_mb = 0
                        try:
                            file_size_mb = workbook_path.stat().st_size / (1024 * 1024)
                        except OSError:
                            pass

                        # –¢–∞–π–º–∞—É—Ç –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –æ–¥–Ω–æ–≥–æ —Ñ–∞–π–ª–∞: 5 –º–∏–Ω—É—Ç
                        try:
                            _, file_matches, error, elapsed_time = future.result(timeout=300)
                        except FutureTimeoutError:
                            error_msg = "–¢–∞–π–º–∞—É—Ç –æ–±—Ä–∞–±–æ—Ç–∫–∏ (–ø—Ä–µ–≤—ã—à–µ–Ω–æ 5 –º–∏–Ω—É—Ç)"
                            logger.error(
                                f"‚è±Ô∏è –¢–∞–π–º–∞—É—Ç –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ñ–∞–π–ª–∞ {workbook_path.name} (–ø—Ä–µ–≤—ã—à–µ–Ω–æ 5 –º–∏–Ω—É—Ç), –ø—Ä–æ–ø—É—Å–∫–∞–µ–º"
                            )
                            get_error_logger().log_search_error(
                                file_path=workbook_path,
                                error_message=error_msg,
                                file_size_mb=file_size_mb,
                                processing_time=300.0,
                            )
                            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –ø—Ä–æ–±–ª–µ–º–Ω–æ–º —Ñ–∞–π–ª–µ
                            failed_files.append({
                                "path": str(workbook_path),
                                "error": error_msg,
                                "file_size_mb": file_size_mb,
                            })
                            failed_count += 1
                            continue
                        
                        total_elapsed_time += elapsed_time

                        if error:
                            error_msg = str(error)
                            logger.error(
                                f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–∏—Å–∫–µ –ø–æ —Ñ–∞–π–ª—É {workbook_path.name} (—Ä–∞–∑–º–µ—Ä {file_size_mb:.2f} –ú–ë, –≤—Ä–µ–º—è {elapsed_time:.1f} —Å–µ–∫): {error_msg}"
                            )
                            get_error_logger().log_search_error(
                                file_path=workbook_path,
                                error_message=error_msg,
                                file_size_mb=file_size_mb,
                                processing_time=elapsed_time,
                            )
                            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –ø—Ä–æ–±–ª–µ–º–Ω–æ–º —Ñ–∞–π–ª–µ
                            failed_files.append({
                                "path": str(workbook_path),
                                "error": error_msg,
                                "file_size_mb": file_size_mb,
                            })
                            failed_count += 1
                        else:
                            if elapsed_time > 120:
                                logger.warning(
                                    f"–§–∞–π–ª {workbook_path.name} –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–ª—Å—è –¥–æ–ª–≥–æ: {elapsed_time:.1f} —Å–µ–∫ (—Ä–∞–∑–º–µ—Ä {file_size_mb:.2f} –ú–ë)"
                                )

                            match_info = f"–Ω–∞–π–¥–µ–Ω–æ {len(file_matches)} —Å–æ–≤–ø–∞–¥–µ–Ω–∏–π" if file_matches else "—Å–æ–≤–ø–∞–¥–µ–Ω–∏–π –Ω–µ –Ω–∞–π–¥–µ–Ω–æ"
                            # –õ–æ–≥–∏—Ä—É–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ score —Å–æ–≤–ø–∞–¥–µ–Ω–∏–π –¥–ª—è –æ—Ç–ª–∞–¥–∫–∏
                            if file_matches:
                                scores = [m.get("score", 0) for m in file_matches]
                                min_score = min(scores) if scores else 0
                                max_score = max(scores) if scores else 0
                                avg_score = sum(scores) / len(scores) if scores else 0
                                logger.debug(
                                    f"–§–∞–π–ª {workbook_path.name}: –Ω–∞–π–¥–µ–Ω–æ {len(file_matches)} —Å–æ–≤–ø–∞–¥–µ–Ω–∏–π, "
                                    f"score min={min_score:.1f}, max={max_score:.1f}, avg={avg_score:.1f}"
                                )
                            logger.info(
                                f"‚úÖ –ü–æ–∏—Å–∫ –ø–æ –¥–æ–∫—É–º–µ–Ω—Ç—É {workbook_path.name} ({processed_count}/{total_files}) ‚Äî {match_info}, –≤—Ä–µ–º—è {elapsed_time:.1f} —Å–µ–∫, —Ä–∞–∑–º–µ—Ä {file_size_mb:.2f} –ú–ë"
                            )

                            for match in file_matches:
                                # –ù–µ —Ñ–∏–ª—å—Ç—Ä—É–µ–º –ø–æ score –∑–¥–µ—Å—å - –≤—Å–µ –Ω–∞–π–¥–µ–Ω–Ω—ã–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è –¥–æ–ª–∂–Ω—ã –ø–æ–ø–∞—Å—Ç—å –≤ —Ä–µ–∑—É–ª—å—Ç–∞—Ç
                                # –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –ø–æ score –±—É–¥–µ—Ç –ø—Ä–æ–∏—Å—Ö–æ–¥–∏—Ç—å –ø—Ä–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–∏ –≤ –ë–î
                                product_name = match.get("product_name")
                                if not product_name:
                                    continue
                                existing = matches.get(product_name)
                                if not existing or existing.get("score", 0) < match.get("score", 0):
                                    matches[product_name] = {**match, "source_file": str(workbook_path)}

                        remaining_files = total_files - processed_count
                        if remaining_files > 0:
                            estimated_time_per_file = (
                                avg_time_per_file
                                if avg_time_per_file > 0
                                else (total_elapsed_time / processed_count if processed_count > 0 else 0)
                            )
                            estimated_time_per_file_adjusted = estimated_time_per_file / workers if workers > 0 else estimated_time_per_file
                            estimated_remaining_seconds = remaining_files * estimated_time_per_file_adjusted
                            time_str = self._format_eta(estimated_remaining_seconds)
                            logger.info(
                                f"–ü—Ä–æ–≥—Ä–µ—Å—Å: –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ {processed_count}/{total_files} —Ñ–∞–π–ª–æ–≤, –æ—Å—Ç–∞–ª–æ—Å—å –ø—Ä–∏–º–µ—Ä–Ω–æ {time_str}"
                            )

                    except Exception as error:
                        error_msg = str(error)
                        failed_count += 1
                        logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ —Ñ–∞–π–ª–∞ {workbook_path.name}: {error_msg}")
                        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –ø—Ä–æ–±–ª–µ–º–Ω–æ–º —Ñ–∞–π–ª–µ
                        try:
                            file_size_mb = workbook_path.stat().st_size / (1024 * 1024) if workbook_path.exists() else 0
                        except OSError:
                            file_size_mb = 0
                        failed_files.append({
                            "path": str(workbook_path),
                            "error": error_msg,
                            "file_size_mb": file_size_mb,
                        })
                        continue
            
            # –ü–∞—É–∑–∞ –º–µ–∂–¥—É –±–∞—Ç—á–∞–º–∏ —Ñ–∞–π–ª–æ–≤ –¥–ª—è –æ—Ö–ª–∞–∂–¥–µ–Ω–∏—è –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä–∞
            if batch_end < total_files and self.batch_delay > 0:
                remaining_batches = (total_files - batch_end) // batch_size + (1 if (total_files - batch_end) % batch_size > 0 else 0)
                logger.info(f"‚è∏Ô∏è  –ü–∞—É–∑–∞ –ø–æ—Å–ª–µ –ø–∞—Ä—Ç–∏–∏ {batch_number}. –û—Å—Ç–∞–ª–æ—Å—å –ø–∞—Ä—Ç–∏–π: {remaining_batches}. –û—Ö–ª–∞–∂–¥–µ–Ω–∏–µ –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä–∞ {self.batch_delay:.1f} —Å–µ–∫...")
                time.sleep(self.batch_delay)

        logger.info(
            f"–û–±—Ä–∞–±–æ—Ç–∫–∞ —Ñ–∞–π–ª–æ–≤ –∑–∞–≤–µ—Ä—à–µ–Ω–∞: —É—Å–ø–µ—à–Ω–æ {processed_count - failed_count}/{total_files}, –æ—à–∏–±–æ–∫ {failed_count}, —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Å–æ–≤–ø–∞–¥–µ–Ω–∏–π {len(matches)}"
        )
        if failed_files:
            logger.warning(f"‚ö†Ô∏è –û–±–Ω–∞—Ä—É–∂–µ–Ω–æ {len(failed_files)} –ø—Ä–æ–±–ª–µ–º–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤, –∫–æ—Ç–æ—Ä—ã–µ –Ω–µ —É–¥–∞–ª–æ—Å—å –æ–±—Ä–∞–±–æ—Ç–∞—Ç—å")
        
        # #region agent log
        import json
        import time as time_module
        import os
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω—ã–π –ø—É—Ç—å (Path —É–∂–µ –∏–º–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω –≥–ª–æ–±–∞–ª—å–Ω–æ)
        project_root = Path(__file__).parent.parent.parent
        log_path = project_root / ".cursor" / "debug.log"
        log_path.parent.mkdir(parents=True, exist_ok=True)
        try:
            with open(str(log_path), "a", encoding="utf-8") as f:
                f.flush()
                os.fsync(f.fileno())
                f.write(json.dumps({
                    "sessionId": "debug-session",
                    "runId": "run1",
                    "hypothesisId": "B",
                    "location": "match_executor.py:run:return",
                    "message": "MatchExecutor –∑–∞–≤–µ—Ä—à–∏–ª —Ä–∞–±–æ—Ç—É",
                    "data": {
                        "total_files": total_files,
                        "processed_count": processed_count,
                        "failed_count": failed_count,
                        "matches_count": len(matches),
                        "failed_files_count": len(failed_files),
                        "matches_sample": [{"product_name": m.get("product_name"), "score": m.get("score", 0)} for m in list(matches.values())[:5]]
                    },
                    "timestamp": int(time_module.time() * 1000)
                }, ensure_ascii=False) + "\n")
        except Exception:
            pass
        # #endregion
        
        return {
            "matches": list(matches.values()),
            "failed_files": failed_files
        }

    @staticmethod
    def _format_eta(seconds: float) -> str:
        if seconds < 60:
            return f"{int(seconds)} —Å–µ–∫"
        if seconds < 3600:
            minutes = int(seconds / 60)
            sec = int(seconds % 60)
            return f"{minutes} –º–∏–Ω {sec} —Å–µ–∫"
        hours = int(seconds / 3600)
        minutes = int((seconds % 3600) / 60)
        return f"{hours} —á {minutes} –º–∏–Ω"
