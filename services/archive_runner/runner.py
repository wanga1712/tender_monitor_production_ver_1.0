"""
MODULE: services.archive_runner.runner
RESPONSIBILITY: Main orchestrator for background tender document processing.
ALLOWED: TenderDatabaseManager, DocumentSearchService, TenderProcessor, TenderFolderManager, logging.
FORBIDDEN: Direct SQL queries (use repositories/managers).
ERRORS: None.

–û—Å–Ω–æ–≤–Ω–æ–π –º–æ–¥—É–ª—å –∑–∞–ø—É—Å–∫–∞ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ —Ç–æ—Ä–≥–æ–≤.
"""

from __future__ import annotations

import os
import time
import shutil
from pathlib import Path
from typing import Any, Dict, List, Optional, Set, Tuple
from concurrent.futures import ThreadPoolExecutor, as_completed

from loguru import logger

from config.settings import config
from core.database import DatabaseManager
from core.tender_database import TenderDatabaseManager
from core.exceptions import DocumentSearchError, DatabaseConnectionError
from services.document_search_service import DocumentSearchService
from services.document_search.document_selector import DocumentSelector
from services.document_search.document_downloader import DocumentDownloader
from services.document_search.download_timeout_calculator import create_timeout_calculator
from services.document_search.archive_extractor import ArchiveExtractor
from services.document_search.match_finder import MatchFinder
from services.tender_services.tender_repository_facade import TenderRepositoryFacade
from services.match_services.tender_match_repository_facade import TenderMatchRepositoryFacade
from services.archive_runner.file_cleaner import FileCleaner
from services.archive_runner.existing_files_processor import ExistingFilesProcessor
from services.archive_runner.tender_provider import TenderProvider
from services.archive_runner.tender_folder_manager import TenderFolderManager
from services.archive_runner.tender_processor import TenderProcessor
from services.archive_runner.tender_prefetcher import TenderPrefetcher, PrefetchedTenderData
from services.archive_runner.tender_queue_manager import TenderQueueManager
from services.archive_runner.processed_tenders_repository import ProcessedTendersRepository
# from services.storage.yandex_disk import YandexDiskClient  # –í—Ä–µ–º–µ–Ω–Ω–æ–µ –æ—Ç–∫–ª—é—á–µ–Ω–∏–µ

# –ù–æ–≤—ã–µ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã –¥–ª—è –¥–µ–∫–æ–º–ø–æ–∑–∏—Ü–∏–∏
from services.archive_runner.folder_processor import FolderProcessor
from services.archive_runner.cloud_uploader import CloudUploader
from services.archive_runner.error_handler import ErrorHandler
from services.archive_runner.tender_coordinator import TenderCoordinator


class ArchiveBackgroundRunner:
    """
    –ö–æ–æ—Ä–¥–∏–Ω–∞—Ç–æ—Ä —Ñ–æ–Ω–æ–≤–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤:
    1. –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç —É–∂–µ —Å–∫–∞—á–∞–Ω–Ω—ã–µ —Ñ–∞–π–ª—ã
    2. –°–∫–∞—á–∏–≤–∞–µ—Ç –Ω–æ–≤—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã
    3. –ù–∞—Ö–æ–¥–∏—Ç —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è –∏ —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
    """


    def __init__(
        self,
        tender_db_manager: TenderDatabaseManager,
        product_db_manager: DatabaseManager,
        user_id: int = 1,
        max_workers: int = 2,
        batch_size: int = 5,
        batch_delay: float = 10.0,
    ):
        self.tender_db_manager = tender_db_manager
        self.product_db_manager = product_db_manager
        self.user_id = user_id
        self.max_workers = max(1, max_workers)  # –ú–∏–Ω–∏–º—É–º 1 –ø–æ—Ç–æ–∫
        self.batch_size = max(1, batch_size)  # –ú–∏–Ω–∏–º—É–º 1 —Ç–æ—Ä–≥ –≤ –±–∞—Ç—á–µ
        self.batch_delay = max(0.0, batch_delay)  # –ú–∏–Ω–∏–º—É–º 0 —Å–µ–∫—É–Ω–¥ –∑–∞–¥–µ—Ä–∂–∫–∏

        self.tender_repo = TenderRepositoryFacade(tender_db_manager)
        self.tender_match_repo = TenderMatchRepositoryFacade(tender_db_manager)
        self.processed_tenders_repo = ProcessedTendersRepository(tender_db_manager)
        self.tender_provider = TenderProvider(self.tender_repo, user_id)

        download_dir = Path(config.document_download_dir) if config.document_download_dir else Path.home() / "Downloads" / "–ï–ò–°_–î–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è"
        self.download_dir = download_dir
        self.download_dir.mkdir(parents=True, exist_ok=True)
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –Ø–Ω–¥–µ–∫—Å –î–∏—Å–∫–∞ (–µ—Å–ª–∏ –≤–∫–ª—é—á–µ–Ω) - –≤—Ä–µ–º–µ–Ω–Ω–æ –æ—Ç–∫–ª—é—á–µ–Ω–æ
        self.yandex_disk = None
        # if config.yandex_disk.enabled and config.yandex_disk.token:
        #     try:
        #         self.yandex_disk = YandexDiskClient(
        #             token=config.yandex_disk.token,
        #             base_path=config.yandex_disk.base_path
        #         )
        #         if self.yandex_disk.check_connection():
        #             logger.info("‚úÖ –ü–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ –Ø–Ω–¥–µ–∫—Å –î–∏—Å–∫—É —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–æ")
        #         else:
        #             logger.warning("‚ö†Ô∏è  –ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–¥–∫–ª—é—á–∏—Ç—å—Å—è –∫ –Ø–Ω–¥–µ–∫—Å –î–∏—Å–∫—É, –ø—Ä–æ–¥–æ–ª–∂–∞–µ–º –±–µ–∑ –Ω–µ–≥–æ")
        #             self.yandex_disk = None
        #     except Exception as e:
        #         logger.warning(f"‚ö†Ô∏è  –û—à–∏–±–∫–∞ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ –Ø–Ω–¥–µ–∫—Å –î–∏—Å–∫–∞: {e}, –ø—Ä–æ–¥–æ–ª–∂–∞–µ–º –±–µ–∑ –Ω–µ–≥–æ")
        #         self.yandex_disk = None

        self.document_search_service = DocumentSearchService(
            product_db_manager,
            download_dir,
            unrar_path=config.unrar_tool,
            winrar_path=config.winrar_path,
        )
        self.document_search_service.ensure_products_loaded()

        self.selector = DocumentSelector()
        # –°–æ–∑–¥–∞–µ–º –∫–∞–ª—å–∫—É–ª—è—Ç–æ—Ä —Ç–∞–π–º–∞—É—Ç–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –ë–î
        timeout_calculator = create_timeout_calculator(tender_db_manager)
        self.downloader = DocumentDownloader(
            download_dir,
            progress_callback=None,
            timeout_calculator=timeout_calculator
        )
        self.extractor = ArchiveExtractor(
            unrar_path=config.unrar_tool,
            winrar_path=config.winrar_path,
        )
        # –°—Ç–æ–ø-—Ñ—Ä–∞–∑—ã –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–∏ (–º–æ–∂–Ω–æ —Ö—Ä–∞–Ω–∏—Ç—å –Ω–∞ —Å—Ç–æ—Ä–æ–Ω–µ tender_monitor)
        try:
            document_stop_phrases_rows = getattr(self.tender_repo, "get_document_stop_phrases", lambda _uid: [])(user_id)
            document_stop_phrases = [
                row.get("phrase", "").strip()
                for row in document_stop_phrases_rows
                if row.get("phrase")
            ]
        except Exception:
            document_stop_phrases = []

        # –ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏–µ —Ñ—Ä–∞–∑—ã –¥–ª—è –ø–æ–∏—Å–∫–∞ –ø–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–∏
        # –§—Ä–∞–∑—ã –∑–∞–≥—Ä—É–∂–∞—é—Ç—Å—è –∏–∑ additional_phrases.py (–∏–Ω—ä–µ–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ, —É—Å–∏–ª–µ–Ω–∏–µ –∏ —Ç.–¥.)
        # –û–Ω–∏ –æ–±—ä–µ–¥–∏–Ω—è—é—Ç—Å—è —Å –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–º–∏ —Ñ—Ä–∞–∑–∞–º–∏ –≤–Ω—É—Ç—Ä–∏ MatchFinder
        user_search_phrases = []  # –ü–æ–∫–∞ –Ω–µ –∏—Å–ø–æ–ª—å–∑—É–µ–º –ë–î, —Ñ—Ä–∞–∑—ã –±–µ—Ä—É—Ç—Å—è –∏–∑ additional_phrases.py

        self.match_finder = MatchFinder(
            self.document_search_service._product_names,
            stop_phrases=document_stop_phrases,
            user_search_phrases=user_search_phrases,
        )
        self.file_cleaner = FileCleaner()
        self.existing_processor = ExistingFilesProcessor(download_dir)
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –º–µ–Ω–µ–¥–∂–µ—Ä –ø–∞–ø–æ–∫ –∏ –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä —Ç–µ–Ω–¥–µ—Ä–æ–≤
        self.folder_manager = TenderFolderManager(download_dir)
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –Ω–æ–≤—ã—Ö –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤ –¥–ª—è –¥–µ–∫–æ–º–ø–æ–∑–∏—Ü–∏–∏
        self.folder_processor = FolderProcessor(self.folder_manager)
        self.cloud_uploader = CloudUploader(self.yandex_disk)
        self.error_handler = ErrorHandler(max_retries=3, retry_delay=2.0)
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç–æ—Ä–∞
        self.tender_coordinator = TenderCoordinator(
            folder_processor=self.folder_processor,
            cloud_uploader=self.cloud_uploader,
            error_handler=self.error_handler,
            queue_manager=TenderQueueManager(),  # TODO: –ó–∞–º–µ–Ω–∏—Ç—å –Ω–∞ —Ä–µ–∞–ª—å–Ω—ã–π –º–µ–Ω–µ–¥–∂–µ—Ä –æ—á–µ—Ä–µ–¥–µ–π
            max_workers=self.max_workers
        )
        
        self.tender_processor = TenderProcessor(
            tender_match_repo=self.tender_match_repo,
            folder_manager=self.folder_manager,
            document_search_service=self.document_search_service,
            selector=self.selector,
            downloader=self.downloader,
            extractor=self.extractor,
            match_finder=self.match_finder,
            file_cleaner=self.file_cleaner,
            processed_tenders_repo=self.processed_tenders_repo,
            max_workers=self.max_workers,
            safe_call_func=self.error_handler.safe_call,  # –ò—Å–ø–æ–ª—å–∑—É–µ–º ErrorHandler –Ω–∞–ø—Ä—è–º—É—é
            get_avg_time_func=self._get_average_processing_time_per_file,
            batch_delay=min(self.batch_delay, 5.0),  # –ò—Å–ø–æ–ª—å–∑—É–µ–º –º–µ–Ω—å—à—É—é –∑–∞–¥–µ—Ä–∂–∫—É –¥–ª—è —Ñ–∞–π–ª–æ–≤
        )

        self._processed_tenders: Set[Tuple[int, str]] = set()
        self._reconnect_delay = 60

    def run(self, specific_tender_ids: Optional[List[Dict[str, Any]]] = None, registry_type: Optional[str] = None, tender_type: str = 'all') -> Dict[str, Any]:
        """
        –ó–∞–ø—É—Å–∫ –ø–æ–ª–Ω–æ–≥–æ —Ü–∏–∫–ª–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –¥–ª—è –≤—Å–µ—Ö —Ç–∏–ø–æ–≤ —Ç–æ—Ä–≥–æ–≤ –∏–ª–∏ –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–≥–æ —Ç–∏–ø–∞.
        –ï—Å–ª–∏ tender_type='all', –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç 'new', 'won', 'commission'.
        """
        if specific_tender_ids:
            # –ï—Å–ª–∏ —É–∫–∞–∑–∞–Ω—ã –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ ID, –∑–∞–ø—É—Å–∫–∞–µ–º –æ–¥–∏–Ω –ø—Ä–æ—Ö–æ–¥
            pass_type = tender_type if tender_type != 'all' else 'new'
            return self._run_single_pass(specific_tender_ids, registry_type, pass_type)

        if tender_type == 'all':
            types_to_process = ['new', 'won']
        else:
            types_to_process = [tender_type]

        aggregated_results = {
            "existing_folders": 0,
            "total_tenders": 0,
            "processed": 0,
            "errors": 0,
            "total_matches": 0,
            "total_time": 0.0,
        }
        
        start_time = time.time()
        
        for t_type in types_to_process:
            try:
                logger.info(f"üîÑ --- –ù–∞—á–∞–ª–æ —Ü–∏–∫–ª–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –¥–ª—è —Ç–∏–ø–∞: {t_type} ---")
                results = self._run_single_pass(specific_tender_ids, registry_type, t_type)
                
                aggregated_results["existing_folders"] += results.get("existing_folders", 0)
                aggregated_results["total_tenders"] += results.get("total_tenders", 0)
                aggregated_results["processed"] += results.get("processed", 0)
                aggregated_results["errors"] += results.get("errors", 0)
                aggregated_results["total_matches"] += results.get("total_matches", 0)
            except Exception as e:
                logger.error(f"–û—à–∏–±–∫–∞ –≤ —Ü–∏–∫–ª–µ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –¥–ª—è —Ç–∏–ø–∞ {t_type}: {e}")
                aggregated_results["errors"] += 1
                
        aggregated_results["total_time"] = time.time() - start_time
        
        logger.info(f"\n{'='*80}")
        logger.info("üèÅ –û–ë–©–ò–ô –ò–¢–û–ì (–í–°–ï –¢–ò–ü–´)")
        logger.info(f"{'='*80}")
        logger.info(f"‚úÖ –í—Å–µ–≥–æ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ: {aggregated_results['processed']}")
        logger.info(f"‚ùå –í—Å–µ–≥–æ –æ—à–∏–±–æ–∫: {aggregated_results['errors']}")
        logger.info(f"‚è±Ô∏è  –û–±—â–µ–µ –≤—Ä–µ–º—è: {aggregated_results['total_time']:.2f} —Å–µ–∫")
        
        return aggregated_results

    def _run_single_pass(self, specific_tender_ids: Optional[List[Dict[str, Any]]] = None, registry_type: Optional[str] = None, tender_type: str = 'new') -> Dict[str, Any]:
        """
        –ó–∞–ø—É—Å–∫ –ø–æ–ª–Ω–æ–≥–æ —Ü–∏–∫–ª–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏:
        - —Å–Ω–∞—á–∞–ª–∞ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ —Ñ–∞–π–ª—ã
        - –∑–∞—Ç–µ–º –Ω–æ–≤—ã–µ —Ç–æ—Ä–≥–∏ –∏–∑ –ë–î –∏–ª–∏ –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ –∑–∞–∫—É–ø–∫–∏

        Args:
            specific_tender_ids: –°–ø–∏—Å–æ–∫ —Å–ª–æ–≤–∞—Ä–µ–π —Å –∫–ª—é—á–∞–º–∏ 'id' –∏ 'registry_type' –¥–ª—è –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã—Ö –∑–∞–∫—É–ø–æ–∫
            registry_type: –¢–∏–ø —Ä–µ–µ—Å—Ç—Ä–∞ –¥–ª—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏ ('44fz' –∏–ª–∏ '223fz'). –ï—Å–ª–∏ None, –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞—é—Ç—Å—è –æ–±–∞.
            tender_type: –¢–∏–ø —Ç–æ—Ä–≥–æ–≤ ('new' –¥–ª—è –Ω–æ–≤—ã—Ö, 'won' –¥–ª—è —Ä–∞–∑—ã–≥—Ä–∞–Ω–Ω—ã—Ö). –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é 'new'.
        """
        # #region agent log - –î–û logger.info!
        import json
        import time
        import os
        from pathlib import Path
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω—ã–π –ø—É—Ç—å
        project_root = Path(__file__).parent.parent.parent
        log_path = project_root / ".cursor" / "debug.log"
        log_path.parent.mkdir(parents=True, exist_ok=True)

        try:
            with open(str(log_path), "a", encoding="utf-8") as f:
                f.write(json.dumps({
                    "sessionId": "debug-session",
                    "runId": "main",
                    "hypothesisId": "MAIN",
                    "location": "runner.py:run:entry",
                    "message": "–ó–∞–ø—É—Å–∫ ArchiveBackgroundRunner.run",
                    "data": {
                        "specific_tender_ids_count": len(specific_tender_ids) if specific_tender_ids else 0,
                        "registry_type": registry_type,
                        "tender_type": tender_type
                    },
                    "timestamp": int(time.time() * 1000)
}, ensure_ascii=False) + "\n")
            f.flush()
            os.fsync(f.fileno())
        except Exception as e:
            # –ó–∞–ø–∏—à–µ–º –≤ stderr –µ—Å–ª–∏ –ª–æ–≥ –Ω–µ –ø–∏—à–µ—Ç—Å—è
            print(f"Failed to write log: {e}", file=__import__('sys').stderr)
        # #endregion

        logger.info("üöÄ –ó–∞–ø—É—Å–∫ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ —Ç–æ—Ä–≥–æ–≤")
        logger.info("=" * 80)
        logger.info(f"–ü–∞—Ä–∞–º–µ—Ç—Ä—ã –∑–∞–ø—É—Å–∫–∞: specific_tenders={specific_tender_ids is not None}, registry_type={registry_type}, tender_type={tender_type}")
        try:
            with open(str(log_path), "a", encoding="utf-8") as f:
                f.write(json.dumps({
                    "sessionId": "debug-session",
                    "runId": "subprocess",
                    "hypothesisId": "PROCESS",
                    "location": "runner.py:run:entry",
                    "message": "ArchiveBackgroundRunner.run –∑–∞–ø—É—â–µ–Ω",
                    "data": {
                        "specific_tender_ids_count": len(specific_tender_ids) if specific_tender_ids else 0,
                        "registry_type": registry_type,
                        "tender_type": tender_type
                    },
                    "timestamp": int(time.time() * 1000)
                }, ensure_ascii=False) + "\n")
                f.flush()
                os.fsync(f.fileno())
        except Exception:
            pass
        # #endregion

        overall_start = time.time()

        # #region agent log
        try:
            with open(str(log_path), "a", encoding="utf-8") as f:
                f.write(json.dumps({
                    "sessionId": "debug-session",
                    "runId": "main",
                    "hypothesisId": "MAIN",
                    "location": "runner.py:run:after_init",
                    "message": "–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞, –Ω–∞—á–∏–Ω–∞–µ–º –æ–±—Ä–∞–±–æ—Ç–∫—É",
                    "data": {
                        "overall_start": overall_start
                    },
                    "timestamp": int(time.time() * 1000)
                }, ensure_ascii=False) + "\n")
                f.flush()
                os.fsync(f.fileno())
        except Exception:
            pass
        # #endregion

        # #region agent log
        try:
            with open(str(log_path), "a", encoding="utf-8") as f:
                f.write(json.dumps({
                    "sessionId": "debug-session",
                    "runId": "main",
                    "hypothesisId": "MAIN",
                    "location": "runner.py:run:before_process_existing",
                    "message": "–ü–µ—Ä–µ–¥ –≤—ã–∑–æ–≤–æ–º _process_existing_folders",
                    "data": {
                        "registry_type": registry_type,
                        "tender_type": tender_type
                    },
                    "timestamp": int(time.time() * 1000)
                }, ensure_ascii=False) + "\n")
                f.flush()
                os.fsync(f.fileno())
        except Exception:
            pass
        # #endregion

        # #region agent log - CHECK FOR REPROCESSING
        try:
            with open(str(log_path), "a", encoding="utf-8") as f:
                f.write(json.dumps({
                    "sessionId": "debug-session",
                    "runId": "reprocessing_check",
                    "hypothesisId": "REPROCESSING",
                    "location": "runner.py:run:before_existing_folders",
                    "message": "–ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –ø–æ–≤—Ç–æ—Ä–Ω—É—é –æ–±—Ä–∞–±–æ—Ç–∫—É",
                    "data": {
                        "specific_tenders_count": len(specific_tender_ids) if specific_tender_ids else 0,
                        "registry_type": registry_type,
                        "tender_type": tender_type
                    },
                    "timestamp": int(time.time() * 1000)
                }, ensure_ascii=False) + "\n")
                f.flush()
                os.fsync(f.fileno())
        except Exception:
            pass
        # #endregion

        # –°–æ–±–∏—Ä–∞–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –ø–∞–ø–∫–∏ –¥–ª—è –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏
        existing_folders: List[Dict[str, Any]] = []
        try:
            logger.info("–°–±–æ—Ä —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö –ø–∞–ø–æ–∫ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏...")
            existing_entries = self.existing_processor.list_pending_tenders()
            logger.info(f"–ù–∞–π–¥–µ–Ω–æ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–π —Å —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–º–∏ —Ñ–∞–π–ª–∞–º–∏: {len(existing_entries)}")
            
            # –ë–∞—Ç—á-–ø—Ä–æ–≤–µ—Ä–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã—Ö —Ç–æ—Ä–≥–æ–≤ –¥–ª—è —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö —Ñ–∞–π–ª–æ–≤
            tenders_by_registry: Dict[str, List[int]] = {}
            for entry in existing_entries:
                if registry_type and entry.get("registry_type") != registry_type:
                    continue
                entry_tender_type = entry.get("tender_type", "new")
                if entry_tender_type != tender_type:
                    continue
                reg = entry.get("registry_type", "44fz")
                tender_id = entry.get("tender_id")
                if tender_id:
                    if reg not in tenders_by_registry:
                        tenders_by_registry[reg] = []
                    tenders_by_registry[reg].append(tender_id)
            
            processed_tenders_cache_existing: Dict[Tuple[int, str], Dict[str, Any]] = {}
            for reg, tender_ids in tenders_by_registry.items():
                batch_results = self._safe_tender_call(
                    self.tender_match_repo.get_match_results_batch,
                    tender_ids,
                    reg,
                )
                for tender_id, match_result in batch_results.items():
                    processed_tenders_cache_existing[(tender_id, reg)] = match_result
            
            filtered_entries = [e for e in existing_entries if (not registry_type or e.get("registry_type") == registry_type) and e.get("tender_type", "new") == tender_type]
            logger.info(f"–ù–∞–π–¥–µ–Ω–æ {len(filtered_entries)} —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö –ø–∞–ø–æ–∫ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ (—Ñ–∏–ª—å—Ç—Ä: registry_type={registry_type}, tender_type={tender_type})")
            
            # –°–æ–±–∏—Ä–∞–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –ø–∞–ø–∫–∏ —Å —Ä–∞–∑–º–µ—Ä–∞–º–∏ –∏ —Å–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ —Ä–∞–∑–º–µ—Ä—É (—Å–∞–º–∞—è –º–∞–ª–µ–Ω—å–∫–∞—è –ø–µ—Ä–≤–∞—è)
            for entry in filtered_entries:
                key = (entry["tender_id"], entry["registry_type"])
                folder_path = entry.get("folder_path")
                folder_name = folder_path.name if folder_path else None
                
                # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º —É–∂–µ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–µ
                if key in processed_tenders_cache_existing:
                    match_result = processed_tenders_cache_existing[key]
                    if folder_name and match_result.get("folder_name") == folder_name:
                        continue
                    elif match_result:
                        continue
                
                # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –ø–æ folder_name
                if folder_name:
                    folder_match_result = self._safe_tender_call(
                        self.tender_match_repo.get_match_result_by_folder_name,
                        folder_name,
                    )
                    if folder_match_result:
                        continue
                
                # –í—ã—á–∏—Å–ª—è–µ–º —Ä–∞–∑–º–µ—Ä –ø–∞–ø–∫–∏
                folder_size = 0
                if folder_path and folder_path.exists():
                    folder_size = self.folder_manager.get_folder_size(folder_path)
                
                existing_folders.append({
                    "id": entry["tender_id"],
                    "registry_type": entry["registry_type"],
                    "folder_path": folder_path,
                    "tender_type": entry.get("tender_type", "new"),
                    "folder_size": folder_size,
                })
            
            # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ —Ä–∞–∑–º–µ—Ä—É (—Å–∞–º–∞—è –º–∞–ª–µ–Ω—å–∫–∞—è –ø–µ—Ä–≤–∞—è)
            existing_folders.sort(key=lambda x: x["folder_size"])
            logger.info(f"–ü–æ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω–æ {len(existing_folders)} —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö –ø–∞–ø–æ–∫ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ (–æ—Ç—Å–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω–æ –ø–æ —Ä–∞–∑–º–µ—Ä—É)")
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–±–æ—Ä–µ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö —Ñ–∞–π–ª–æ–≤: {e}")
            logger.exception("–î–µ—Ç–∞–ª–∏ –æ—à–∏–±–∫–∏:")
            existing_folders = []

        logger.info(f"–ü–æ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω–æ {len(existing_folders)} —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö –ø–∞–ø–æ–∫ –¥–ª—è –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏")
        
        # #region agent log
        try:
            with open(str(log_path), "a", encoding="utf-8") as f:
                f.write(json.dumps({
                    "sessionId": "debug-session",
                    "runId": "subprocess",
                    "hypothesisId": "PROCESS",
                    "location": "runner.py:run:before_get_tenders",
                    "message": "–ü–µ—Ä–µ–¥ –ø–æ–ª—É—á–µ–Ω–∏–µ–º —Å–ø–∏—Å–∫–∞ —Ç–æ—Ä–≥–æ–≤",
                    "data": {
                        "specific_tender_ids_count": len(specific_tender_ids) if specific_tender_ids else 0,
                        "registry_type": registry_type,
                        "tender_type": tender_type
                    },
                    "timestamp": int(time.time() * 1000)
                }, ensure_ascii=False) + "\n")
                f.flush()
                os.fsync(f.fileno())
        except Exception:
            pass
        # #endregion

        # –ï—Å–ª–∏ —É–∫–∞–∑–∞–Ω—ã –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ –∑–∞–∫—É–ø–∫–∏, –∏—Å–ø–æ–ª—å–∑—É–µ–º –∏—Ö, –∏–Ω–∞—á–µ –ø–æ–ª—É—á–∞–µ–º –ø–æ –Ω–∞—Å—Ç—Ä–æ–π–∫–∞–º
        logger.info("–ü–æ–ª—É—á–µ–Ω–∏–µ —Å–ø–∏—Å–∫–∞ —Ç–æ—Ä–≥–æ–≤ –∏–∑ –ë–î...")
        if specific_tender_ids:
            logger.info(f"–ó–∞–ø—Ä–æ—à–µ–Ω—ã –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ –∑–∞–∫—É–ø–∫–∏: {len(specific_tender_ids)} —à—Ç.")
            tenders = self._safe_tender_call(
                self.tender_provider.get_target_tenders,
                specific_tender_ids=specific_tender_ids,
                registry_type=registry_type,
                tender_type=tender_type
            )
        else:
            logger.info("–ü–æ–ª—É—á–µ–Ω–∏–µ –≤—Å–µ—Ö —Ç–æ—Ä–≥–æ–≤ –ø–æ –Ω–∞—Å—Ç—Ä–æ–π–∫–∞–º –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è...")
            tenders = self._safe_tender_call(
                self.tender_provider.get_target_tenders,
                registry_type=registry_type,
                tender_type=tender_type
            )
        logger.info(f"–ü–æ–ª—É—á–µ–Ω–æ —Ç–æ—Ä–≥–æ–≤ –∏–∑ –ë–î: {len(tenders) if tenders else 0}")

        # #region agent log
        try:
            with open(str(log_path), "a", encoding="utf-8") as f:
                f.write(json.dumps({
                    "sessionId": "debug-session",
                    "runId": "subprocess",
                    "hypothesisId": "PROCESS",
                    "location": "runner.py:run:after_get_tenders",
                    "message": "–ü–æ—Å–ª–µ –ø–æ–ª—É—á–µ–Ω–∏—è —Å–ø–∏—Å–∫–∞ —Ç–æ—Ä–≥–æ–≤",
                    "data": {
                        "tenders_count": len(tenders) if tenders else 0,
                        "tenders_is_none": tenders is None,
                        "specific_tender_ids_count": len(specific_tender_ids) if specific_tender_ids else 0,
                        "registry_type": registry_type,
                        "tender_type": tender_type
                    },
                    "timestamp": int(time.time() * 1000)
                }, ensure_ascii=False) + "\n")
                f.flush()
                os.fsync(f.fileno())
        except Exception:
            pass
        # #endregion
        
        if not tenders:
            logger.warning("–ù–µ—Ç —Ç–æ—Ä–≥–æ–≤ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏")
            # #region agent log
            try:
                with open(str(log_path), "a", encoding="utf-8") as f:
                    f.write(json.dumps({
                        "sessionId": "debug-session",
                        "runId": "subprocess",
                        "hypothesisId": "PROCESS",
                        "location": "runner.py:run:no_tenders",
                        "message": "–ù–µ—Ç —Ç–æ—Ä–≥–æ–≤ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏",
                        "data": {
                            "tenders_is_none": tenders is None,
                            "tenders_is_empty": tenders == [] if tenders is not None else None
                        },
                        "timestamp": int(time.time() * 1000)
                    }, ensure_ascii=False) + "\n")
                    f.flush()
                    os.fsync(f.fileno())
            except Exception:
                pass
            # #endregion

        # #region agent log
        try:
            with open(str(log_path), "a", encoding="utf-8") as f:
                f.write(json.dumps({
                    "sessionId": "debug-session",
                    "runId": "subprocess",
                    "hypothesisId": "PROCESS",
                    "location": "runner.py:run:before_batch_check",
                    "message": "–ü–µ—Ä–µ–¥ –±–∞—Ç—á-–ø—Ä–æ–≤–µ—Ä–∫–æ–π –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã—Ö —Ç–æ—Ä–≥–æ–≤",
                    "data": {
                        "tenders_count": len(tenders) if tenders else 0
                        },
                        "timestamp": int(time.time() * 1000)
                    }, ensure_ascii=False) + "\n")
                f.flush()
                os.fsync(f.fileno())
        except Exception:
                pass
            # #endregion

        # –ë–∞—Ç—á-–ø—Ä–æ–≤–µ—Ä–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã—Ö —Ç–æ—Ä–≥–æ–≤ –¥–ª—è –Ω–æ–≤—ã—Ö —Ç–æ—Ä–≥–æ–≤
        processed_tenders_cache: Dict[Tuple[int, str], Dict[str, Any]] = {}
        if tenders:
            tenders_by_registry: Dict[str, List[int]] = {}
            for tender in tenders:
                registry = tender.get("registry_type", "44fz")
                tender_id = tender.get("id")
                if tender_id:
                    if registry not in tenders_by_registry:
                        tenders_by_registry[registry] = []
                    tenders_by_registry[registry].append(tender_id)
            
            for registry, tender_ids in tenders_by_registry.items():
                batch_results = self._safe_tender_call(
                    self.tender_match_repo.get_match_results_batch,
                    tender_ids,
                    registry,
                )
                for tender_id, match_result in batch_results.items():
                    processed_tenders_cache[(tender_id, registry)] = match_result
            
            logger.info(f"–ë–∞—Ç—á-–ø—Ä–æ–≤–µ—Ä–∫–∞ –Ω–æ–≤—ã—Ö —Ç–æ—Ä–≥–æ–≤: –∏–∑ {len(tenders)} —É–∂–µ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ: {len(processed_tenders_cache)}")

            # #region agent log
            try:
                with open(str(log_path), "a", encoding="utf-8") as f:
                    f.write(json.dumps({
                        "sessionId": "debug-session",
                        "runId": "subprocess",
                        "hypothesisId": "PROCESS",
                        "location": "runner.py:run:after_batch_check",
                        "message": "–ü–æ—Å–ª–µ –±–∞—Ç—á-–ø—Ä–æ–≤–µ—Ä–∫–∏ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã—Ö —Ç–æ—Ä–≥–æ–≤",
                        "data": {
                            "processed_cache_size": len(processed_tenders_cache),
                            "tenders_count": len(tenders) if tenders else 0
                        },
                        "timestamp": int(time.time() * 1000)
                    }, ensure_ascii=False) + "\n")
                    f.flush()
                    os.fsync(f.fileno())
            except Exception:
                pass
            # #endregion

        # –ï—Å–ª–∏ –∑–∞–ø—Ä–æ—à–µ–Ω—ã –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ –∑–∞–∫—É–ø–∫–∏ ‚Äî —Ñ–æ—Ä—Å–∏—Ä—É–µ–º –ø–æ–≤—Ç–æ—Ä–Ω—É—é –æ–±—Ä–∞–±–æ—Ç–∫—É (–Ω–µ –¥–æ–≤–µ—Ä—è–µ–º –∫—ç—à—É)
        if specific_tender_ids:
            logger.info(
                f"–§–æ—Ä—Å–∏—Ä—É–µ–º –æ–±—Ä–∞–±–æ—Ç–∫—É –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã—Ö –∑–∞–∫—É–ø–æ–∫: —Å–±—Ä–∞—Å—ã–≤–∞–µ–º –∫—ç—à –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã—Ö ({len(processed_tenders_cache)})"
            )
            processed_tenders_cache = {}

            # #region agent log
            try:
                with open(str(log_path), "a", encoding="utf-8") as f:
                    f.write(json.dumps({
                        "sessionId": "debug-session",
                        "runId": "subprocess",
                        "hypothesisId": "PROCESS",
                        "location": "runner.py:run:force_reprocess_specific",
                        "message": "FORCE_REPROCESS_SPECIFIC",
                        "data": {
                            "specific_tender_ids_count": len(specific_tender_ids),
                            "processed_cache_size_before": len(processed_tenders_cache)
                        },
                        "timestamp": int(time.time() * 1000)
                    }, ensure_ascii=False) + "\n")
                    f.flush()
                    os.fsync(f.fileno())
            except Exception:
                pass
            # #endregion

        # –ü–æ–ª—É—á–∞–µ–º —Å—Ä–µ–¥–Ω–µ–µ –≤—Ä–µ–º—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –æ–¥–Ω–æ–≥–æ —Ç–æ—Ä–≥–∞ –∏–∑ –ë–î –¥–ª—è —Ä–∞—Å—á–µ—Ç–∞ –æ—Å—Ç–∞–≤—à–µ–≥–æ—Å—è –≤—Ä–µ–º–µ–Ω–∏
        avg_time_per_tender = self._get_average_processing_time_per_tender()

        # #region agent log
        try:
            with open(str(log_path), "a", encoding="utf-8") as f:
                f.write(json.dumps({
                    "sessionId": "debug-session",
                    "runId": "subprocess",
                    "hypothesisId": "PROCESS",
                    "location": "runner.py:run:before_parallel_processing",
                    "message": "–ü–µ—Ä–µ–¥ –∑–∞–ø—É—Å–∫–æ–º –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏",
                    "data": {
                        "avg_time_per_tender": avg_time_per_tender,
                        "tenders_count": len(tenders) if tenders else 0
                    },
                    "timestamp": int(time.time() * 1000)
                }, ensure_ascii=False) + "\n")
                f.flush()
                os.fsync(f.fileno())
        except Exception:
            pass
        # #endregion

        # –ó–∞–ø—É—Å–∫–∞–µ–º –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω—É—é –æ–±—Ä–∞–±–æ—Ç–∫—É —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö –ø–∞–ø–æ–∫ –∏ —Å–∫–∞—á–∏–≤–∞–Ω–∏–µ –Ω–æ–≤—ã—Ö —Ç–æ—Ä–≥–æ–≤
        logger.info("üöÄ –ó–∞–ø—É—Å–∫ –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö –ø–∞–ø–æ–∫ –∏ —Å–∫–∞—á–∏–≤–∞–Ω–∏—è –Ω–æ–≤—ã—Ö —Ç–æ—Ä–≥–æ–≤")
        
        # –°–æ–∑–¥–∞—ë–º prefetcher –¥–ª—è –Ω–æ–≤—ã—Ö —Ç–æ—Ä–≥–æ–≤
        prefetcher: Optional[TenderPrefetcher] = None
        if tenders:
            # #region agent log
            try:
                with open(str(log_path), "a", encoding="utf-8") as f:
                    f.write(json.dumps({
                        "sessionId": "debug-session",
                        "runId": "subprocess",
                        "hypothesisId": "PROCESS",
                        "location": "runner.py:run:before_create_prefetcher",
                        "message": "–ü–µ—Ä–µ–¥ —Å–æ–∑–¥–∞–Ω–∏–µ–º prefetcher",
                        "data": {
                            "tender_type": tender_type
                        },
                        "timestamp": int(time.time() * 1000)
                    }, ensure_ascii=False) + "\n")
                    f.flush()
                    os.fsync(f.fileno())
            except Exception:
                pass
            # #endregion

            prefetcher = self._create_prefetcher(tender_type)

            # #region agent log
            try:
                with open(str(log_path), "a", encoding="utf-8") as f:
                    f.write(json.dumps({
                        "sessionId": "debug-session",
                        "runId": "subprocess",
                        "hypothesisId": "PROCESS",
                        "location": "runner.py:run:after_create_prefetcher",
                        "message": "–ü–æ—Å–ª–µ —Å–æ–∑–¥–∞–Ω–∏—è prefetcher",
                        "data": {
                            "prefetcher_created": prefetcher is not None
                        },
                        "timestamp": int(time.time() * 1000)
                    }, ensure_ascii=False) + "\n")
                    f.flush()
                    os.fsync(f.fileno())
            except Exception:
                pass
            # #endregion

            prefetcher.schedule(tenders, self._get_tender_documents_safe)

            # #region agent log
            try:
                with open(str(log_path), "a", encoding="utf-8") as f:
                    f.write(json.dumps({
                        "sessionId": "debug-session",
                        "runId": "subprocess",
                        "hypothesisId": "PROCESS",
                        "location": "runner.py:run:after_schedule_prefetcher",
                        "message": "–ü–æ—Å–ª–µ schedule prefetcher",
                        "data": {
                            "tenders_count": len(tenders) if tenders else 0
                        },
                        "timestamp": int(time.time() * 1000)
                    }, ensure_ascii=False) + "\n")
                    f.flush()
                    os.fsync(f.fileno())
            except Exception:
                pass
            # #endregion
        
        # –°—á–µ—Ç—á–∏–∫–∏ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏
        processed = 0
        errors = 0
        skipped_no_docs = 0
        total_matches = 0
        
        # –ó–∞–ø—É—Å–∫–∞–µ–º –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω—É—é –æ–±—Ä–∞–±–æ—Ç–∫—É —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö –ø–∞–ø–æ–∫
        if existing_folders:
            logger.info(f"–ù–∞—á–∏–Ω–∞–µ–º –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω—É—é –æ–±—Ä–∞–±–æ—Ç–∫—É {len(existing_folders)} —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö –ø–∞–ø–æ–∫")
            existing_stats = self._process_existing_folders_parallel(
                existing_folders,
                processed_tenders_cache,
                tender_type,
                prefetcher,
                tenders if tenders else []
            )
            processed += existing_stats.get("processed", 0)
            errors += existing_stats.get("errors", 0)
            total_matches += existing_stats.get("total_matches", 0)
        
        # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –Ω–æ–≤—ã–µ —Ç–æ—Ä–≥–∏ –∏–∑ –æ—á–µ—Ä–µ–¥–∏ —Å–∫–∞—á–∏–≤–∞–Ω–∏—è
        if tenders:
            logger.info(f"–ù–∞—á–∏–Ω–∞–µ–º –æ–±—Ä–∞–±–æ—Ç–∫—É {len(tenders)} –Ω–æ–≤—ã—Ö —Ç–æ—Ä–≥–æ–≤")
            new_stats = self._process_new_tenders(
                tenders,
                processed_tenders_cache,
                tender_type,
                prefetcher,
            )
            processed += new_stats.get("processed", 0)
            errors += new_stats.get("errors", 0)
            skipped_no_docs += new_stats.get("skipped_no_docs", 0)
            total_matches += new_stats.get("total_matches", 0)

        if prefetcher:
            try:
                prefetcher.shutdown()
            except Exception as e:
                logger.warning(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–∫—Ä—ã—Ç–∏–∏ prefetcher: {e}", exc_info=True)

        overall_time = time.time() - overall_start

        logger.info(f"\n{'='*80}")
        logger.info("üìä –ò–¢–û–ì–û–í–ê–Ø –°–¢–ê–¢–ò–°–¢–ò–ö–ê")
        logger.info(f"{'='*80}")
        logger.info(f"üìÅ –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–π: {len(existing_folders)}")
        logger.info(f"üì¶ –ù–æ–≤—ã—Ö —Ç–æ—Ä–≥–æ–≤: {len(tenders) if tenders else 0}")
        logger.info(f"‚úÖ –£—Å–ø–µ—à–Ω–æ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ: {processed}")
        logger.info(f"‚è≠Ô∏è –ü—Ä–æ–ø—É—â–µ–Ω–æ (–Ω–µ—Ç –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤): {skipped_no_docs}")
        logger.info(f"‚ùå –û—à–∏–±–æ–∫: {errors}")
        logger.info(f"üîç –í—Å–µ–≥–æ –Ω–∞–π–¥–µ–Ω–æ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–π: {total_matches}")
        logger.info(f"‚è±Ô∏è  –û–±—â–µ–µ –≤—Ä–µ–º—è: {overall_time:.2f} —Å–µ–∫")

        return {
            "existing_folders": len(existing_folders),
            "total_tenders": len(tenders) if tenders else 0,
            "processed": processed,
            "errors": errors,
            "total_matches": total_matches,
            "total_time": overall_time,
        }

    def _process_existing_folders(self, registry_type: Optional[str] = None, tender_type: str = 'new') -> int:
        """–û–±—Ä–∞–±–æ—Ç–∫–∞ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö –ø–∞–ø–æ–∫ —Å –¥–æ–∫—É–º–µ–Ω—Ç–∞–º–∏"""
        # #region agent log
        import json
        import time
        from pathlib import Path
        log_path = Path(__file__).parent.parent.parent / ".cursor" / "debug.log"
        log_path.parent.mkdir(parents=True, exist_ok=True)
        try:
            with open(str(log_path), "a", encoding="utf-8") as f:
                f.write(json.dumps({
                    "sessionId": "debug-session",
                    "runId": "main",
                    "hypothesisId": "MAIN",
                    "location": "runner.py:_process_existing_folders:entry",
                    "message": "–ù–∞—á–∏–Ω–∞–µ–º –æ–±—Ä–∞–±–æ—Ç–∫—É —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö –ø–∞–ø–æ–∫",
                    "data": {
                        "registry_type": registry_type,
                        "tender_type": tender_type
                    },
                    "timestamp": int(time.time() * 1000)
                }, ensure_ascii=False) + "\n")
                f.flush()
                os.fsync(f.fileno())
        except Exception:
            pass
        # #endregion
        """
        –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç —É–∂–µ —Å–∫–∞—á–∞–Ω–Ω—ã–µ —Ñ–∞–π–ª—ã –≤ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è—Ö.
        
        Args:
            registry_type: –¢–∏–ø —Ä–µ–µ—Å—Ç—Ä–∞ –¥–ª—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏ ('44fz' –∏–ª–∏ '223fz')
            tender_type: –¢–∏–ø —Ç–æ—Ä–≥–æ–≤ ('new' –¥–ª—è –Ω–æ–≤—ã—Ö, 'won' –¥–ª—è —Ä–∞–∑—ã–≥—Ä–∞–Ω–Ω—ã—Ö)
        """
        entries = self.existing_processor.list_pending_tenders()
        logger.info(f"–ù–∞–π–¥–µ–Ω–æ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–π —Å —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–º–∏ —Ñ–∞–π–ª–∞–º–∏: {len(entries)}")
        
        if not entries:
            return 0
        
        # –ë–∞—Ç—á-–ø—Ä–æ–≤–µ—Ä–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã—Ö —Ç–æ—Ä–≥–æ–≤ –¥–ª—è —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö —Ñ–∞–π–ª–æ–≤
        tenders_by_registry: Dict[str, List[int]] = {}
        for entry in entries:
            # –§–∏–ª—å—Ç—Ä—É–µ–º –ø–æ registry_type
            if registry_type and entry.get("registry_type") != registry_type:
                continue
            # –§–∏–ª—å—Ç—Ä—É–µ–º –ø–æ tender_type - –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Ç–æ–ª—å–∫–æ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∏–π —Ç–∏–ø
            entry_tender_type = entry.get("tender_type", "new")
            if entry_tender_type != tender_type:
                continue
            reg = entry.get("registry_type", "44fz")
            tender_id = entry.get("tender_id")
            if tender_id:
                if reg not in tenders_by_registry:
                    tenders_by_registry[reg] = []
                tenders_by_registry[reg].append(tender_id)
        
        processed_tenders_cache: Dict[Tuple[int, str], Dict[str, Any]] = {}
        for reg, tender_ids in tenders_by_registry.items():
            batch_results = self._safe_tender_call(
                self.tender_match_repo.get_match_results_batch,
                tender_ids,
                reg,
            )
            for tender_id, match_result in batch_results.items():
                processed_tenders_cache[(tender_id, reg)] = match_result
        
        filtered_count = len([e for e in entries if (not registry_type or e.get("registry_type") == registry_type) and e.get("tender_type", "new") == tender_type])
        logger.info(
            f"–ë–∞—Ç—á-–ø—Ä–æ–≤–µ—Ä–∫–∞ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö —Ñ–∞–π–ª–æ–≤ ({tender_type}): –∏–∑ {filtered_count} —Ç–æ—Ä–≥–æ–≤ —É–∂–µ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ: {len(processed_tenders_cache)}"
        )
        
        processed = 0
        filtered_entries = [e for e in entries if (not registry_type or e.get("registry_type") == registry_type) and e.get("tender_type", "new") == tender_type]
        logger.info(f"–ù–∞—á–∏–Ω–∞–µ–º –æ–±—Ä–∞–±–æ—Ç–∫—É {len(filtered_entries)} —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö –ø–∞–ø–æ–∫ (—Ñ–∏–ª—å—Ç—Ä: registry_type={registry_type}, tender_type={tender_type})")
        
        # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –æ–±—Ä–∞–±–æ—Ç–∫—É —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö —Ñ–∞–π–ª–æ–≤ –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è
        # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Ç–æ–ª—å–∫–æ –ø–µ—Ä–≤—ã–µ 100, –æ—Å—Ç–∞–ª—å–Ω—ã–µ –ø—Ä–æ–ø—É—Å–∫–∞–µ–º (–æ–Ω–∏ —É–∂–µ –æ–±—Ä–∞–±–æ—Ç–∞–Ω—ã –∏–ª–∏ –±—É–¥—É—Ç –æ–±—Ä–∞–±–æ—Ç–∞–Ω—ã –∫–∞–∫ –Ω–æ–≤—ã–µ)
        max_existing_to_process = 100
        entries_to_process = filtered_entries[:max_existing_to_process]
        logger.info(f"–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –ø–µ—Ä–≤—ã–µ {len(entries_to_process)} —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö –ø–∞–ø–æ–∫ (–¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è)")
        
        for idx, entry in enumerate(entries_to_process):
            if idx % 50 == 0 and idx > 0:
                logger.info(f"–û–±—Ä–∞–±–æ—Ç–∫–∞ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö —Ñ–∞–π–ª–æ–≤: {idx}/{len(entries_to_process)}")
            tender = {
                "id": entry["tender_id"],
                "registry_type": entry["registry_type"],
                "folder_path": entry["folder_path"],
            }
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫—ç—à –ø–µ—Ä–µ–¥ –æ–±—Ä–∞–±–æ—Ç–∫–æ–π
            key = (tender["id"], tender["registry_type"])
            folder_path = entry.get("folder_path")
            folder_name = folder_path.name if folder_path else None
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø–æ tender_id –≤ –∫—ç—à–µ
            if key in processed_tenders_cache:
                match_result = processed_tenders_cache[key]
                # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ –ø—Ä–æ–≤–µ—Ä—è–µ–º –ø–æ folder_name, –µ—Å–ª–∏ –æ–Ω —É–∫–∞–∑–∞–Ω
                if folder_name and match_result.get("folder_name") == folder_name:
                    logger.debug(f"–ü–∞–ø–∫–∞ {folder_name} —É–∂–µ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–∞ (–Ω–∞–π–¥–µ–Ω–æ –≤ –ë–î –ø–æ folder_name)")
                    self._processed_tenders.add(key)
                    continue
                elif match_result:
                    # –ï—Å–ª–∏ –µ—Å—Ç—å –∑–∞–ø–∏—Å—å –≤ –ë–î, –Ω–æ folder_name –Ω–µ —Å–æ–≤–ø–∞–¥–∞–µ—Ç, –≤—Å–µ —Ä–∞–≤–Ω–æ –ø—Ä–æ–ø—É—Å–∫–∞–µ–º
                    logger.debug(f"–¢–æ—Ä–≥ {tender['id']} ({tender['registry_type']}) —É–∂–µ –æ–±—Ä–∞–±–æ—Ç–∞–Ω, –Ω–æ folder_name –Ω–µ —Å–æ–≤–ø–∞–¥–∞–µ—Ç. –ü—Ä–æ–ø—É—Å–∫–∞–µ–º.")
                    self._processed_tenders.add(key)
                    continue
            
            # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –ø–æ folder_name –≤ –ë–î (–µ—Å–ª–∏ –Ω–µ –Ω–∞—à–ª–∏ –≤ –∫—ç—à–µ)
            if folder_name:
                folder_match_result = self._safe_tender_call(
                    self.tender_match_repo.get_match_result_by_folder_name,
                    folder_name,
                )
                if folder_match_result:
                    logger.debug(f"–ü–∞–ø–∫–∞ {folder_name} —É–∂–µ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–∞ (–Ω–∞–π–¥–µ–Ω–æ –≤ –ë–î –ø–æ folder_name)")
                    processed_tenders_cache[key] = folder_match_result
                    self._processed_tenders.add(key)
                    continue
            
            documents = self._safe_tender_call(
                self.tender_provider.get_tender_documents,
                tender["id"],
                tender["registry_type"],
            )
            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º tender_type –∏–∑ –ø–∞–ø–∫–∏
            tender_type_from_folder = entry.get("tender_type", "new")
            try:
                folder_path = self.folder_manager.prepare_tender_folder(tender["id"], tender["registry_type"], tender_type_from_folder)
                existing_records = self.existing_processor.build_records(folder_path)
                if not existing_records:
                    continue
                tender["folder_path"] = folder_path
            except Exception as e:
                logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–¥–≥–æ—Ç–æ–≤–∫–µ –ø–∞–ø–∫–∏ –¥–ª—è —Ç–æ—Ä–≥–∞ {tender['id']}: {e}")
                continue
            
            try:
                result = self._process_tender(
                    tender,
                    documents=documents,
                    existing_records=existing_records,
                    processed_tenders_cache=processed_tenders_cache,
                    tender_type=tender_type_from_folder,
                )
                if result:
                    processed += 1
                    self._processed_tenders.add((tender["id"], tender["registry_type"]))
            except KeyboardInterrupt:
                # –ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å –ø—Ä–µ—Ä–≤–∞–ª –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–µ
                raise
            except SystemExit:
                # –°–∏—Å—Ç–µ–º–Ω—ã–π –≤—ã—Ö–æ–¥
                raise
            except Exception as e:
                logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ —Å—É—â–µ—Å—Ç–≤—É—é—â–µ–≥–æ —Ñ–∞–π–ª–∞ –¥–ª—è —Ç–æ—Ä–≥–∞ {tender['id']}: {e}", exc_info=True)
                # –ü—Ä–æ–¥–æ–ª–∂–∞–µ–º –æ–±—Ä–∞–±–æ—Ç–∫—É —Å–ª–µ–¥—É—é—â–∏—Ö
                continue
        
        logger.info(f"–û–±—Ä–∞–±–æ—Ç–∫–∞ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö —Ñ–∞–π–ª–æ–≤ –∑–∞–≤–µ—Ä—à–µ–Ω–∞. –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ: {processed} –∏–∑ {len(entries_to_process)} (–≤—Å–µ–≥–æ –Ω–∞–π–¥–µ–Ω–æ: {len(filtered_entries)})")
        return processed

    def _process_tender(
        self,
        tender: Dict[str, Any],
        documents: Optional[List[Dict[str, Any]]] = None,
        existing_records: Optional[List[Dict[str, Any]]] = None,
        prefetched_data: Optional[PrefetchedTenderData] = None,
        processed_tenders_cache: Optional[Dict[Tuple[int, str], Dict[str, Any]]] = None,
        tender_type: str = 'new',
    ) -> Optional[Dict[str, Any]]:
        """–û–±—Ä–∞–±–æ—Ç–∫–∞ –æ–¥–Ω–æ–≥–æ —Ç–µ–Ω–¥–µ—Ä–∞ (–¥–µ–ª–µ–≥–∏—Ä—É–µ—Ç—Å—è TenderProcessor)"""
        tender_id = tender.get("id")
        registry_type = tender.get("registry_type", "44fz")
        folder_path = prefetched_data.folder_path if prefetched_data else self.folder_manager.prepare_tender_folder(tender_id, registry_type, tender_type)
        tender["folder_path"] = folder_path

        return self.tender_processor.process_tender(
            tender=tender,
            documents=documents,
            existing_records=existing_records,
            prefetched_data=prefetched_data,
            processed_tenders_cache=processed_tenders_cache,
            tender_type=tender_type,
            get_tender_documents_func=lambda tid, rt: self._safe_tender_call(
                self.tender_provider.get_tender_documents,
                tid,
                rt,
            ),
        )

    def _get_tender_documents_safe(self, tender_id: int, registry_type: str) -> List[Dict[str, Any]]:
        """–û–±–µ—Ä—Ç–∫–∞ –¥–ª—è –±–µ–∑–æ–ø–∞—Å–Ω–æ–≥–æ –ø–æ–ª—É—á–µ–Ω–∏—è –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ —Ç–æ—Ä–≥–∞."""
        return self._safe_tender_call(
                self.tender_provider.get_tender_documents,
                tender_id,
                registry_type,
            )

    def _prepare_tenders_with_sizes(
        self,
        tenders: List[Dict[str, Any]],
        prefetcher: Optional[TenderPrefetcher],
        tender_type: str = 'new',
    ) -> Tuple[List[Tuple[Dict[str, Any], int]], Dict[int, int]]:
        """
        –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ—Ç —Å–ø–∏—Å–æ–∫ —Ç–µ–Ω–¥–µ—Ä–æ–≤ —Å —Ä–∞–∑–º–µ—Ä–∞–º–∏ –ø–∞–ø–æ–∫ –∏ —Å–æ—Ä—Ç–∏—Ä—É–µ—Ç –ø–æ —Ä–∞–∑–º–µ—Ä—É (–æ—Ç –º–µ–Ω—å—à–µ–≥–æ –∫ –±–æ–ª—å—à–µ–º—É).
        
        Args:
            tenders: –°–ø–∏—Å–æ–∫ —Ç–µ–Ω–¥–µ—Ä–æ–≤ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏
            prefetcher: –ü—Ä–µ—Ñ–µ—Ç—á–µ—Ä –¥–ª—è —Å–∫–∞—á–∏–≤–∞–Ω–∏—è –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ (–µ—Å–ª–∏ –µ—Å—Ç—å)
            tender_type: –¢–∏–ø —Ç–æ—Ä–≥–æ–≤ ('new' –∏–ª–∏ 'won')
            
        Returns:
            –ö–æ—Ä—Ç–µ–∂ –∏–∑:
            - –°–ø–∏—Å–æ–∫ –∫–æ—Ä—Ç–µ–∂–µ–π (tender, folder_size), –æ—Ç—Å–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –ø–æ —Ä–∞–∑–º–µ—Ä—É –ø–∞–ø–∫–∏
            - –°–ª–æ–≤–∞—Ä—å –º–∞–ø–ø–∏–Ω–≥–∞ id(tender) -> original_index –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è prefetched_data
        """
        if not tenders:
            return []
        
        logger.info("üì¶ –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ä–∞–∑–º–µ—Ä–æ–≤ –ø–∞–ø–æ–∫ –¥–ª—è –ø—Ä–∏–æ—Ä–∏—Ç–∏–∑–∞—Ü–∏–∏ –æ–±—Ä–∞–±–æ—Ç–∫–∏...")
        
        # –î–æ–∂–∏–¥–∞–µ–º—Å—è —Å–∫–∞—á–∏–≤–∞–Ω–∏—è –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –¥–ª—è –≤—Å–µ—Ö —Ç–µ–Ω–¥–µ—Ä–æ–≤ —á–µ—Ä–µ–∑ prefetcher
        if prefetcher:
            logger.info(f"–û–∂–∏–¥–∞–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è —Å–∫–∞—á–∏–≤–∞–Ω–∏—è –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –¥–ª—è {len(tenders)} —Ç–µ–Ω–¥–µ—Ä–æ–≤...")
            # –ü–æ–ª—É—á–∞–µ–º prefetched_data –¥–ª—è –≤—Å–µ—Ö —Ç–µ–Ω–¥–µ—Ä–æ–≤, —á—Ç–æ–±—ã –¥–æ–∂–¥–∞—Ç—å—Å—è —Å–∫–∞—á–∏–≤–∞–Ω–∏—è
            for idx, tender in enumerate(tenders):
                try:
                    prefetcher.get_prefetched_data(idx, tender)
                except Exception as e:
                    logger.debug(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–ª—É—á–µ–Ω–∏–∏ prefetched_data –¥–ª—è —Ç–æ—Ä–≥–∞ {tender.get('id')}: {e}")
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ä–∞–∑–º–µ—Ä—ã –ø–∞–ø–æ–∫ –¥–ª—è –≤—Å–µ—Ö —Ç–µ–Ω–¥–µ—Ä–æ–≤ –∏ —Å–æ—Ö—Ä–∞–Ω—è–µ–º –º–∞–ø–ø–∏–Ω–≥ –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã—Ö –∏–Ω–¥–µ–∫—Å–æ–≤
        tenders_with_sizes: List[Tuple[Dict[str, Any], int]] = []
        original_index_map: Dict[int, int] = {}  # id(tender) -> original_index
        
        for original_index, tender in enumerate(tenders):
            tender_id = tender.get("id")
            registry_type = tender.get("registry_type", "44fz")
            folder_path = self.folder_manager.prepare_tender_folder(tender_id, registry_type, tender_type)
            folder_size = self.folder_manager.get_folder_size(folder_path)
            tenders_with_sizes.append((tender, folder_size))
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–∞–ø–ø–∏–Ω–≥ –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è prefetched_data
            original_index_map[id(tender)] = original_index
            
            # –õ–æ–≥–∏—Ä—É–µ–º —Ä–∞–∑–º–µ—Ä –¥–ª—è –æ—Ç–ª–∞–¥–∫–∏
            size_mb = folder_size / (1024 * 1024)
            logger.debug(f"–¢–æ—Ä–≥ {tender_id} ({registry_type}): —Ä–∞–∑–º–µ—Ä –ø–∞–ø–∫–∏ {size_mb:.2f} –ú–ë")
        
        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ —Ä–∞–∑–º–µ—Ä—É –ø–∞–ø–∫–∏ (–æ—Ç –º–µ–Ω—å—à–µ–≥–æ –∫ –±–æ–ª—å—à–µ–º—É)
        tenders_with_sizes.sort(key=lambda x: x[1])
        
        # –õ–æ–≥–∏—Ä—É–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
        if tenders_with_sizes:
            min_size_mb = tenders_with_sizes[0][1] / (1024 * 1024)
            max_size_mb = tenders_with_sizes[-1][1] / (1024 * 1024)
            avg_size_mb = sum(size for _, size in tenders_with_sizes) / len(tenders_with_sizes) / (1024 * 1024)
            logger.info(
                f"‚úÖ –ü—Ä–∏–æ—Ä–∏—Ç–∏–∑–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞: {len(tenders_with_sizes)} —Ç–µ–Ω–¥–µ—Ä–æ–≤, "
                f"—Ä–∞–∑–º–µ—Ä—ã –æ—Ç {min_size_mb:.2f} –ú–ë –¥–æ {max_size_mb:.2f} –ú–ë (—Å—Ä–µ–¥–Ω–∏–π: {avg_size_mb:.2f} –ú–ë)"
            )
            logger.info("üìã –û–±—Ä–∞–±–æ—Ç–∫–∞ –±—É–¥–µ—Ç –≤—ã–ø–æ–ª–Ω—è—Ç—å—Å—è –æ—Ç –º–µ–Ω—å—à–∏—Ö –ø–∞–ø–æ–∫ –∫ –±–æ–ª—å—à–∏–º –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ –Ω–∞–ø–æ–ª–Ω–µ–Ω–∏—è –ë–î")
        
        return tenders_with_sizes, original_index_map

    def _create_prefetcher(self, tender_type: str = 'new') -> TenderPrefetcher:
        """–°–æ–∑–¥–∞–µ—Ç –Ω–∞—Å—Ç—Ä–æ–µ–Ω–Ω—ã–π –ø—Ä–µ—Ñ–µ—Ç—á–µ—Ä –¥–ª—è —Ñ–æ–Ω–æ–≤–æ–π –∑–∞–≥—Ä—É–∑–∫–∏ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤."""
        prefetch_size = min(3, max(1, self.max_workers // 2))
        return TenderPrefetcher(
            folder_manager=self.folder_manager,
            selector=self.selector,
            downloader=self.downloader,
            max_prefetch=prefetch_size,
            tender_type=tender_type,
        )


    def _safe_tender_call(self, func, *args, **kwargs):
        """–ë–µ–∑–æ–ø–∞—Å–Ω—ã–π –≤—ã–∑–æ–≤ —Ñ—É–Ω–∫—Ü–∏–∏ —Å –æ–±—Ä–∞–±–æ—Ç–∫–æ–π –æ—à–∏–±–æ–∫ —á–µ—Ä–µ–∑ ErrorHandler"""
        return self.error_handler.safe_call(func, *args, **kwargs)

    def _ensure_tender_connection(self):
        if self.tender_db_manager.is_connected():
            return
        self._attempt_connect()

    def _attempt_connect(self):
        try:
            self.tender_db_manager.connect()
        except DatabaseConnectionError as error:
            self._handle_db_disconnect(error)

    def _get_average_processing_time_per_file(self) -> float:
        """
        –ü–æ–ª—É—á–∞–µ—Ç —Å—Ä–µ–¥–Ω–µ–µ –≤—Ä–µ–º—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –æ–¥–Ω–æ–≥–æ —Ñ–∞–π–ª–∞ –∏–∑ –∏—Å—Ç–æ—Ä–∏–∏ –ë–î.
        
        Returns:
            –°—Ä–µ–¥–Ω–µ–µ –≤—Ä–µ–º—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –æ–¥–Ω–æ–≥–æ —Ñ–∞–π–ª–∞ –≤ —Å–µ–∫—É–Ω–¥–∞—Ö, –∏–ª–∏ 0.0 –µ—Å–ª–∏ –¥–∞–Ω–Ω—ã—Ö –Ω–µ—Ç
        """
        try:
            from psycopg2.extras import RealDictCursor
            query = """
                SELECT 
                    AVG(processing_time_seconds / NULLIF(total_files_processed, 0)) as avg_time_per_file
                FROM tender_document_matches
                WHERE processing_time_seconds IS NOT NULL 
                    AND total_files_processed > 0
                    AND processing_time_seconds > 0
            """
            results = self.tender_db_manager.execute_query(query, None, RealDictCursor)
            if results and results[0].get('avg_time_per_file'):
                avg_time = float(results[0]['avg_time_per_file'])
                logger.debug(f"–°—Ä–µ–¥–Ω–µ–µ –≤—Ä–µ–º—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –æ–¥–Ω–æ–≥–æ —Ñ–∞–π–ª–∞ –∏–∑ –∏—Å—Ç–æ—Ä–∏–∏: {avg_time:.2f} —Å–µ–∫")
                return avg_time
        except Exception as error:
            logger.debug(f"–ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å —Å—Ä–µ–¥–Ω–µ–µ –≤—Ä–µ–º—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∏–∑ –ë–î: {error}")
        
        return 0.0
    
    def _get_average_processing_time_per_tender(self) -> float:
        """
        –ü–æ–ª—É—á–∞–µ—Ç —Å—Ä–µ–¥–Ω–µ–µ –≤—Ä–µ–º—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –æ–¥–Ω–æ–≥–æ —Ç–æ—Ä–≥–∞ –∏–∑ –∏—Å—Ç–æ—Ä–∏–∏ –ë–î.
        
        Returns:
            –°—Ä–µ–¥–Ω–µ–µ –≤—Ä–µ–º—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –æ–¥–Ω–æ–≥–æ —Ç–æ—Ä–≥–∞ –≤ —Å–µ–∫—É–Ω–¥–∞—Ö, –∏–ª–∏ 0.0 –µ—Å–ª–∏ –¥–∞–Ω–Ω—ã—Ö –Ω–µ—Ç
        """
        try:
            from psycopg2.extras import RealDictCursor
            query = """
                SELECT 
                    AVG(processing_time_seconds) as avg_time_per_tender
                FROM tender_document_matches
                WHERE processing_time_seconds IS NOT NULL 
                    AND processing_time_seconds > 0
            """
            results = self.tender_db_manager.execute_query(query, None, RealDictCursor)
            if results and results[0].get('avg_time_per_tender'):
                avg_time = float(results[0]['avg_time_per_tender'])
                logger.debug(f"–°—Ä–µ–¥–Ω–µ–µ –≤—Ä–µ–º—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –æ–¥–Ω–æ–≥–æ —Ç–æ—Ä–≥–∞ –∏–∑ –∏—Å—Ç–æ—Ä–∏–∏: {avg_time:.2f} —Å–µ–∫")
                return avg_time
        except Exception as error:
            logger.debug(f"–ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å —Å—Ä–µ–¥–Ω–µ–µ –≤—Ä–µ–º—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ç–æ—Ä–≥–∞ –∏–∑ –ë–î: {error}")
        
        return 0.0
    
    def _process_existing_folders_parallel(
        self,
        existing_folders: List[Dict[str, Any]],
        processed_tenders_cache: Dict[Tuple[int, str], Dict[str, Any]],
        tender_type: str,
        prefetcher: Optional[TenderPrefetcher],
        new_tenders: List[Dict[str, Any]]
    ) -> None:
        """
        –ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö –ø–∞–ø–æ–∫.
        –ü–æ–∫–∞ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞—é—Ç—Å—è —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –ø–∞–ø–∫–∏, —Å–∫–∞—á–∏–≤–∞—é—Ç—Å—è –Ω–æ–≤—ã–µ —Ç–æ—Ä–≥–∏.
        
        –õ–æ–≥–∏–∫–∞:
        1. –ë–µ—Ä–µ–º —Å–∞–º—É—é –º–∞–ª–µ–Ω—å–∫—É—é –ø–∞–ø–∫—É
        2. –ü—Ä–æ–±—É–µ–º –æ—Ç–∫—Ä—ã—Ç—å/–æ–±—Ä–∞–±–æ—Ç–∞—Ç—å
        3. –ï—Å–ª–∏ –Ω–µ –æ—Ç–∫—Ä—ã–ª–æ—Å—å - —É–¥–∞–ª—è–µ–º —Ñ–∞–π–ª—ã, –ø–∞–ø–∫—É, —Å–∫–∞—á–∏–≤–∞–µ–º –∑–∞–Ω–æ–≤–æ
        4. –ü–æ–∫–∞ —Å–∫–∞—á–∏–≤–∞–µ–º - –ø—Ä–æ–≤–µ—Ä—è–µ–º –¥—Ä—É–≥–∏–µ –ø–∞–ø–∫–∏
        5. –ï—Å–ª–∏ –æ—Ç–∫—Ä—ã–ª–∞—Å—å - –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º, –∑–∞–ø–∏—Å—ã–≤–∞–µ–º –≤ –ë–î, —É–¥–∞–ª—è–µ–º –ø–∞–ø–∫—É
        """
        from threading import Lock
        
        processed_count = 0
        errors_count = 0
        lock = Lock()
        failed_tenders: List[Dict[str, Any]] = []  # –¢–æ—Ä–≥–∏, –∫–æ—Ç–æ—Ä—ã–µ –Ω—É–∂–Ω–æ —Å–∫–∞—á–∞—Ç—å –∑–∞–Ω–æ–≤–æ
        
        def process_single_folder(folder_data: Dict[str, Any]) -> Optional[Dict[str, Any]]:
            """–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –æ–¥–Ω—É —Å—É—â–µ—Å—Ç–≤—É—é—â—É—é –ø–∞–ø–∫—É"""
            nonlocal processed_count, errors_count
            
            tender_id = folder_data["id"]
            registry_type = folder_data["registry_type"]
            folder_path = folder_data["folder_path"]
            folder_size_mb = folder_data["folder_size"] / (1024 * 1024)
            
            key = (tender_id, registry_type)
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫—ç—à
            if key in processed_tenders_cache:
                logger.debug(f"–¢–æ—Ä–≥ {tender_id} ({registry_type}) —É–∂–µ –æ–±—Ä–∞–±–æ—Ç–∞–Ω, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º")
                return None
            
            logger.info(f"–û–±—Ä–∞–±–æ—Ç–∫–∞ —Å—É—â–µ—Å—Ç–≤—É—é—â–µ–π –ø–∞–ø–∫–∏ {folder_path.name} (—Ä–∞–∑–º–µ—Ä: {folder_size_mb:.2f} –ú–ë)")
            
            try:
                # –ü—Ä–æ–±—É–µ–º —Å–æ–±—Ä–∞—Ç—å –∑–∞–ø–∏—Å–∏ –∏–∑ –ø–∞–ø–∫–∏
                existing_records = self.existing_processor.build_records(folder_path)
                if not existing_records:
                    logger.warning(f"–î–ª—è —Ç–æ—Ä–≥–∞ {tender_id} ({registry_type}) –Ω–µ –Ω–∞–π–¥–µ–Ω–æ —Ñ–∞–π–ª–æ–≤ –≤ –ø–∞–ø–∫–µ {folder_path.name}")
                    # –£–¥–∞–ª—è–µ–º –ø–∞–ø–∫—É –∏ —Å—Ç–∞–≤–∏–º –Ω–∞ —Å–∫–∞—á–∏–≤–∞–Ω–∏–µ
                    self._delete_folder_and_schedule_download(tender_id, registry_type, folder_path, folder_data.get("tender_type", "new"))
                    with lock:
                        errors_count += 1
                    return {"error": "no_files"}
                
                # –°–æ–∑–¥–∞–µ–º —Ç–æ—Ä–≥ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏
                tender = {
                    "id": tender_id,
                    "registry_type": registry_type,
                    "folder_path": folder_path,
                    "tender_type": folder_data.get("tender_type", "new"),
                }
                
                # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Ç–æ—Ä–≥
                result = self._process_tender(
                    tender,
                    existing_records=existing_records,
                    processed_tenders_cache=processed_tenders_cache,
                    tender_type=folder_data.get("tender_type", "new")
                )
                
                if result and not result.get("error"):
                    # –£—Å–ø–µ—à–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ - —É–¥–∞–ª—è–µ–º –ø–∞–ø–∫—É
                    logger.info(f"‚úÖ –¢–æ—Ä–≥ {tender_id} ({registry_type}) —É—Å–ø–µ—à–Ω–æ –æ–±—Ä–∞–±–æ—Ç–∞–Ω, —É–¥–∞–ª—è–µ–º –ø–∞–ø–∫—É")
                    self._delete_folder_after_processing(folder_path, tender_id, registry_type)
                    with lock:
                        processed_count += 1
                    return result
                else:
                    # –û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ - —É–¥–∞–ª—è–µ–º –ø–∞–ø–∫—É –∏ —Å—Ç–∞–≤–∏–º –Ω–∞ —Å–∫–∞—á–∏–≤–∞–Ω–∏–µ
                    logger.warning(f"‚ùå –û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ç–æ—Ä–≥–∞ {tender_id} ({registry_type}), —É–¥–∞–ª—è–µ–º –ø–∞–ø–∫—É –∏ —Å—Ç–∞–≤–∏–º –Ω–∞ —Å–∫–∞—á–∏–≤–∞–Ω–∏–µ")
                    self._delete_folder_and_schedule_download(tender_id, registry_type, folder_path, folder_data.get("tender_type", "new"))
                    with lock:
                        errors_count += 1
                    return {"error": "processing_failed"}
                    
            except Exception as e:
                logger.error(f"‚ùå –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ —Å—É—â–µ—Å—Ç–≤—É—é—â–µ–π –ø–∞–ø–∫–∏ {folder_path.name}: {e}", exc_info=True)
                # –£–¥–∞–ª—è–µ–º –ø–∞–ø–∫—É –∏ —Å—Ç–∞–≤–∏–º –Ω–∞ —Å–∫–∞—á–∏–≤–∞–Ω–∏–µ
                self._delete_folder_and_schedule_download(tender_id, registry_type, folder_path, folder_data.get("tender_type", "new"))
                with lock:
                    errors_count += 1
                return {"error": str(e)}
        
        # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –ø–∞–ø–∫–∏ –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ
        with ThreadPoolExecutor(max_workers=self.max_workers) as executor:
            # #region agent log
            import json
            import os
            project_root = os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
            log_path = os.path.join(project_root, ".cursor", "debug.log")
            try:
                with open(log_path, "a", encoding="utf-8") as f:
                    f.write(json.dumps({
                        "sessionId": "debug-session",
                        "runId": "transaction-debug",
                        "hypothesisId": "THREAD_POOL_IDLE",
                        "location": "runner.py:process_existing_folders:thread_pool_start",
                        "message": "–ó–∞–ø—É—Å–∫ ThreadPoolExecutor –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –ø–∞–ø–æ–∫",
                        "data": {"max_workers": self.max_workers, "folders_count": len(existing_folders)},
                        "timestamp": int(__import__('time').time() * 1000)
                    }) + "\n")
            except Exception:
                pass
            # #endregion

            futures = [executor.submit(process_single_folder, folder) for folder in existing_folders]
            
            for future in as_completed(futures):
                try:
                    result = future.result()
                    if result and result.get("error") == "processing_failed":
                        # –î–æ–±–∞–≤–ª—è–µ–º –≤ —Å–ø–∏—Å–æ–∫ –¥–ª—è —Å–∫–∞—á–∏–≤–∞–Ω–∏—è
                        folder_data = existing_folders[futures.index(future)]
                        failed_tenders.append({
                            "id": folder_data["id"],
                            "registry_type": folder_data["registry_type"],
                            "tender_type": folder_data.get("tender_type", "new"),
                        })
                except Exception as e:
                    logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–ª—É—á–µ–Ω–∏–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –ø–∞–ø–∫–∏: {e}")
        
        logger.info(f"–û–±—Ä–∞–±–æ—Ç–∫–∞ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö –ø–∞–ø–æ–∫ –∑–∞–≤–µ—Ä—à–µ–Ω–∞: –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ {processed_count}, –æ—à–∏–±–æ–∫ {errors_count}")
        
        # –î–æ–±–∞–≤–ª—è–µ–º –Ω–µ—É–¥–∞—á–Ω—ã–µ —Ç–æ—Ä–≥–∏ –≤ –æ—á–µ—Ä–µ–¥—å –Ω–∞ —Å–∫–∞—á–∏–≤–∞–Ω–∏–µ
        if failed_tenders and prefetcher:
            logger.info(f"–î–æ–±–∞–≤–ª—è–µ–º {len(failed_tenders)} —Ç–æ—Ä–≥–æ–≤ –≤ –æ—á–µ—Ä–µ–¥—å –Ω–∞ —Å–∫–∞—á–∏–≤–∞–Ω–∏–µ")
            prefetcher.schedule(failed_tenders, self._get_tender_documents_safe)
        
        return {
            "processed": processed_count,
            "errors": errors_count,
            "total_matches": 0,  # –ë—É–¥–µ—Ç –æ–±–Ω–æ–≤–ª–µ–Ω–æ –∏–∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –æ–±—Ä–∞–±–æ—Ç–∫–∏
        }
    
    def _process_new_tenders(
        self,
        tenders: List[Dict[str, Any]],
        processed_tenders_cache: Dict[Tuple[int, str], Dict[str, Any]],
        tender_type: str,
        prefetcher: Optional[TenderPrefetcher],
    ) -> Dict[str, Any]:
        """–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –Ω–æ–≤—ã–µ —Ç–æ—Ä–≥–∏ –∏–∑ –æ—á–µ—Ä–µ–¥–∏ —Å–∫–∞—á–∏–≤–∞–Ω–∏—è"""
        queue_manager = TenderQueueManager(self.folder_manager, tender_type)
        queue_manager.add_tenders(tenders)
        
        original_index_map: Dict[int, int] = {}
        for idx, tender in enumerate(tenders):
            original_index_map[id(tender)] = idx
        
        processed = 0
        errors = 0
        index = 0
        
        while queue_manager.has_more():
            try:
                next_item = queue_manager.get_next_tender()
                if next_item is None:
                    break
                
                tender, folder_size = next_item
                tender_id = tender.get("id")
                registry_type = tender.get("registry_type", "44fz")
                key = (tender_id, registry_type)
                
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫—ç—à
                if key in processed_tenders_cache:
                    queue_manager.mark_processed()
                    index += 1
                    continue
                
                # –ü–æ–ª—É—á–∞–µ–º prefetched_data
                original_index = original_index_map.get(id(tender), index)
                prefetched_data = prefetcher.get_prefetched_data(original_index, tender) if prefetcher else None
                
                # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Ç–æ—Ä–≥
                # #region agent log
                try:
                    from pathlib import Path
                    import time as _time
                    import json
                    log_path_local = Path(__file__).parent.parent.parent / ".cursor" / "debug.log"
                    with open(log_path_local, "a", encoding="utf-8") as f:
                        f.write(json.dumps({
                            "sessionId": "debug-session",
                            "runId": "subprocess",
                            "hypothesisId": "PROCESS",
                            "location": "runner.py:_process_new_tenders:before_process_tender",
                            "message": "PROCESS_NEW_TENDER_START",
                            "data": {
                                "tender_id": tender_id,
                                "registry_type": registry_type,
                                "tender_type": tender_type,
                                "folder_size_mb": round(folder_size / (1024 * 1024), 2),
                                "prefetched_has_data": prefetched_data is not None,
                                "queue_remaining": queue_manager._remaining
                            },
                            "timestamp": int(_time.time() * 1000)
                        }, ensure_ascii=False) + "\n")
                except Exception:
                    pass
                # #endregion

                result = self._process_tender(
                    tender,
                    prefetched_data=prefetched_data,
                    processed_tenders_cache=processed_tenders_cache,
                    tender_type=tender_type,
                )
                
                queue_manager.mark_processed()
                
                if result and not result.get("error"):
                    # –ó–∞–≥—Ä—É–∂–∞–µ–º –Ω–∞ –Ø–Ω–¥–µ–∫—Å –î–∏—Å–∫ –∏ —É–¥–∞–ª—è–µ–º –ø–∞–ø–∫—É –ø–æ—Å–ª–µ —É—Å–ø–µ—à–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏
                    folder_path = self.folder_manager.get_tender_folder_path(tender_id, registry_type, tender_type)
                    if folder_path and folder_path.exists():
                        self._delete_folder_after_processing(folder_path, tender_id, registry_type)
                    processed += 1
                else:
                    errors += 1
                    # #region agent log
                    try:
                        from pathlib import Path
                        import time as _time
                        import json
                        log_path_local = Path(__file__).parent.parent.parent / ".cursor" / "debug.log"
                        with open(log_path_local, "a", encoding="utf-8") as f:
                            f.write(json.dumps({
                                "sessionId": "debug-session",
                                "runId": "subprocess",
                                "hypothesisId": "PROCESS",
                                "location": "runner.py:_process_new_tenders:process_tender_error",
                                "message": "PROCESS_NEW_TENDER_ERROR",
                                "data": {
                                    "tender_id": tender_id,
                                    "registry_type": registry_type,
                                    "tender_type": tender_type,
                                    "result": result
                                },
                                "timestamp": int(_time.time() * 1000)
                            }, ensure_ascii=False) + "\n")
                    except Exception:
                        pass
                    # #endregion
                
                index += 1
            except Exception as e:
                logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ –Ω–æ–≤–æ–≥–æ —Ç–æ—Ä–≥–∞: {e}", exc_info=True)
                errors += 1
                queue_manager.mark_processed()
                index += 1
        
        logger.info(f"–û–±—Ä–∞–±–æ—Ç–∫–∞ –Ω–æ–≤—ã—Ö —Ç–æ—Ä–≥–æ–≤ –∑–∞–≤–µ—Ä—à–µ–Ω–∞: –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ {processed}, –æ—à–∏–±–æ–∫ {errors}")
        
        return {
            "processed": processed,
            "errors": errors,
            "skipped_no_docs": 0,
            "total_matches": 0,
        }
    
    def _upload_folder_to_yandex_disk(self, folder_path: Path, tender_id: int, registry_type: str) -> bool:
        """
        –ó–∞–≥—Ä—É–∂–∞–µ—Ç –ø–∞–ø–∫—É —Å –¥–æ–∫—É–º–µ–Ω—Ç–∞–º–∏ –Ω–∞ –Ø–Ω–¥–µ–∫—Å –î–∏—Å–∫.
        –ò—Å–ø–æ–ª—å–∑—É–µ—Ç CloudUploader –¥–ª—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è –æ–ø–µ—Ä–∞—Ü–∏–∏.
        
        Args:
            folder_path: –ü—É—Ç—å –∫ –ª–æ–∫–∞–ª—å–Ω–æ–π –ø–∞–ø–∫–µ
            tender_id: ID —Ç–æ—Ä–≥–∞
            registry_type: –¢–∏–ø —Ä–µ–µ—Å—Ç—Ä–∞ (44fz –∏–ª–∏ 223fz)
            
        Returns:
            True –µ—Å–ª–∏ –∑–∞–≥—Ä—É–∑–∫–∞ —É—Å–ø–µ—à–Ω–∞, False –∏–Ω–∞—á–µ
        """
        return self.cloud_uploader.upload_folder_to_yandex_disk(folder_path, tender_id, registry_type)
    
    def _delete_folder_after_processing(self, folder_path: Path, tender_id: int = None, registry_type: str = None) -> None:
        """
        –£–¥–∞–ª—è–µ—Ç –ø–∞–ø–∫—É –ø–æ—Å–ª–µ —É—Å–ø–µ—à–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏.
        –ü–µ—Ä–µ–¥ —É–¥–∞–ª–µ–Ω–∏–µ–º –∑–∞–≥—Ä—É–∂–∞–µ—Ç –Ω–∞ –Ø–Ω–¥–µ–∫—Å –î–∏—Å–∫ (–µ—Å–ª–∏ –≤–∫–ª—é—á–µ–Ω–æ).
        """
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –Ω–∞ –Ø–Ω–¥–µ–∫—Å –î–∏—Å–∫ –ø–µ—Ä–µ–¥ —É–¥–∞–ª–µ–Ω–∏–µ–º (–µ—Å–ª–∏ –≤–∫–ª—é—á–µ–Ω–æ)
        if tender_id and registry_type:
            self._upload_folder_to_yandex_disk(folder_path, tender_id, registry_type)
        
        try:
            if folder_path.exists():
                self.folder_manager.clean_tender_folder_force(folder_path)
                folder_path.rmdir()
                logger.debug(f"–ü–∞–ø–∫–∞ {folder_path.name} —É—Å–ø–µ—à–Ω–æ —É–¥–∞–ª–µ–Ω–∞ –ø–æ—Å–ª–µ –æ–±—Ä–∞–±–æ—Ç–∫–∏")
        except Exception as e:
            logger.warning(f"–ù–µ —É–¥–∞–ª–æ—Å—å —É–¥–∞–ª–∏—Ç—å –ø–∞–ø–∫—É {folder_path.name} –ø–æ—Å–ª–µ –æ–±—Ä–∞–±–æ—Ç–∫–∏: {e}")
    
    def _delete_folder_and_schedule_download(
        self,
        tender_id: int,
        registry_type: str,
        folder_path: Path,
        tender_type: str
    ) -> None:
        """–£–¥–∞–ª—è–µ—Ç –ø–∞–ø–∫—É –∏ –ø–ª–∞–Ω–∏—Ä—É–µ—Ç —Å–∫–∞—á–∏–≤–∞–Ω–∏–µ –∑–∞–Ω–æ–≤–æ"""
        try:
            if folder_path.exists():
                # –ò—Å–ø–æ–ª—å–∑—É–µ–º FolderProcessor –¥–ª—è —É–¥–∞–ª–µ–Ω–∏—è –ø–∞–ø–∫–∏
                self.folder_processor.delete_folder_after_processing(folder_path, tender_id, registry_type)
                logger.info(f"–ü–∞–ø–∫–∞ {folder_path.name} —É–¥–∞–ª–µ–Ω–∞, —Ç–æ—Ä–≥ {tender_id} –±—É–¥–µ—Ç —Å–∫–∞—á–∞–Ω –∑–∞–Ω–æ–≤–æ")
        except Exception as e:
            logger.warning(f"–ù–µ —É–¥–∞–ª–æ—Å—å —É–¥–∞–ª–∏—Ç—å –ø–∞–ø–∫—É {folder_path.name}: {e}")
    
    def _handle_failed_existing_tender(self, tender: Dict[str, Any], queue_manager: TenderQueueManager) -> None:
        """
        –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç —Å–ª—É—á–∞–π, –∫–æ–≥–¥–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∞ —Å—É—â–µ—Å—Ç–≤—É—é—â–µ–≥–æ —Ñ–∞–π–ª–∞ –Ω–µ —É–¥–∞–ª–∞—Å—å.
        –£–¥–∞–ª—è–µ—Ç –ø–∞–ø–∫—É –∏ –¥–æ–±–∞–≤–ª—è–µ—Ç —Ç–æ—Ä–≥ –≤ –æ—á–µ—Ä–µ–¥—å –Ω–∞ —Å–∫–∞—á–∏–≤–∞–Ω–∏–µ.
        
        Args:
            tender: –°–ª–æ–≤–∞—Ä—å —Å –¥–∞–Ω–Ω—ã–º–∏ —Ç–æ—Ä–≥–∞
            queue_manager: –ú–µ–Ω–µ–¥–∂–µ—Ä –æ—á–µ—Ä–µ–¥–∏ –¥–ª—è –¥–æ–±–∞–≤–ª–µ–Ω–∏—è —Ç–æ—Ä–≥–∞ –Ω–∞ —Å–∫–∞—á–∏–≤–∞–Ω–∏–µ
        """
        tender_id = tender.get("id")
        registry_type = tender.get("registry_type", "44fz")
        folder_path = tender.get("folder_path")
        
        try:
            # –£–¥–∞–ª—è–µ–º –ø–∞–ø–∫—É —Å—É—â–µ—Å—Ç–≤—É—é—â–µ–≥–æ —Ñ–∞–π–ª–∞
            if folder_path and folder_path.exists():
                logger.info(f"–£–¥–∞–ª–µ–Ω–∏–µ –ø–∞–ø–∫–∏ —Å—É—â–µ—Å—Ç–≤—É—é—â–µ–≥–æ —Ñ–∞–π–ª–∞ –¥–ª—è —Ç–æ—Ä–≥–∞ {tender_id} ({registry_type}): {folder_path}")
                # –ò—Å–ø–æ–ª—å–∑—É–µ–º FolderProcessor –¥–ª—è —É–¥–∞–ª–µ–Ω–∏—è –ø–∞–ø–∫–∏
                self.folder_processor.delete_folder_after_processing(folder_path, tender_id, registry_type)
                logger.info(f"–ü–∞–ø–∫–∞ {folder_path.name} —É—Å–ø–µ—à–Ω–æ —É–¥–∞–ª–µ–Ω–∞")
            
            # –°–æ–∑–¥–∞—ë–º –Ω–æ–≤—ã–π —Ç–æ—Ä–≥ –±–µ–∑ –ø–æ–º–µ—Ç–∫–∏ _is_existing –¥–ª—è —Å–∫–∞—á–∏–≤–∞–Ω–∏—è
            new_tender = {
                "id": tender_id,
                "registry_type": registry_type,
                "tender_type": tender.get("tender_type", "new"),
                # –£–±–∏—Ä–∞–µ–º –ø–æ–º–µ—Ç–∫—É _is_existing –∏ folder_path
            }
            
            # –î–æ–±–∞–≤–ª—è–µ–º –≤ –æ—á–µ—Ä–µ–¥—å –Ω–∞ —Å–∫–∞—á–∏–≤–∞–Ω–∏–µ
            queue_manager.add_tenders([new_tender])
            logger.info(f"–¢–æ—Ä–≥ {tender_id} ({registry_type}) –¥–æ–±–∞–≤–ª–µ–Ω –≤ –æ—á–µ—Ä–µ–¥—å –Ω–∞ —Å–∫–∞—á–∏–≤–∞–Ω–∏–µ")
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ –Ω–µ—É–¥–∞—á–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Å—É—â–µ—Å—Ç–≤—É—é—â–µ–≥–æ —Ñ–∞–π–ª–∞ –¥–ª—è —Ç–æ—Ä–≥–∞ {tender_id} ({registry_type}): {e}")
    
    @staticmethod
    def _format_eta(seconds: float) -> str:
        """–§–æ—Ä–º–∞—Ç–∏—Ä—É–µ—Ç –≤—Ä–µ–º—è –≤ —á–∏—Ç–∞–µ–º—ã–π —Ñ–æ—Ä–º–∞—Ç."""
        if seconds < 60:
            return f"{int(seconds)} —Å–µ–∫"
        if seconds < 3600:
            minutes = int(seconds / 60)
            sec = int(seconds % 60)
            return f"{minutes} –º–∏–Ω {sec} —Å–µ–∫"
        hours = int(seconds / 3600)
        minutes = int((seconds % 3600) / 60)
        return f"{hours} —á {minutes} –º–∏–Ω"
    
    def _handle_db_disconnect(self, error: Exception):
        """–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –æ—Ç–∫–ª—é—á–µ–Ω–∏–µ –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö —Å –ø–æ–º–æ—â—å—é ErrorHandler"""
        self.error_handler.handle_db_disconnect(error)

