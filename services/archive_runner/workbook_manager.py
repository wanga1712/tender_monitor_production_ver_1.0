"""
MODULE: services.archive_runner.workbook_manager
RESPONSIBILITY: Prepare document files for processing (unzip, convert, deduplicate).
ALLOWED: DocumentSelector, ArchiveExtractor, DocumentDownloader, ExcelPreparator, ArchiveProcessor, logging.
FORBIDDEN: Business logic (focus on file prep).
ERRORS: None.

–ú–µ–Ω–µ–¥–∂–µ—Ä –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∏ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏.

–ö–æ–æ—Ä–¥–∏–Ω–∏—Ä—É–µ—Ç —Ä–∞–±–æ—Ç—É —Å —Ñ–∞–π–ª–∞–º–∏, –∞—Ä—Ö–∏–≤–∞–º–∏ –∏ Excel —Ñ–∞–π–ª–∞–º–∏.
"""

from __future__ import annotations

from pathlib import Path
from typing import Any, Dict, List, Optional, Set, Tuple

from loguru import logger

from services.document_search.document_selector import DocumentSelector
from services.document_search.archive_extractor import ArchiveExtractor
from services.document_search.document_downloader import DocumentDownloader
from services.archive_runner.file_deduplicator import add_file_to_dict
from services.archive_runner.excel_preparator import ExcelPreparator
from services.archive_runner.archive_processor import ArchiveProcessor


class WorkbookManager:
    """–û—Ç–≤–µ—á–∞–µ—Ç –∑–∞ —Ä–∞—Å–ø–∞–∫–æ–≤–∫—É –∞—Ä—Ö–∏–≤–æ–≤ –∏ –ø–æ–¥–≥–æ—Ç–æ–≤–∫—É –ø—É—Ç–µ–π –∫ –¥–æ–∫—É–º–µ–Ω—Ç–∞–º."""

    ARCHIVE_EXTENSIONS = {".rar", ".zip", ".7z"}
    EXCEL_EXTENSIONS = {".xlsx", ".xls"}
    WORD_EXTENSIONS = {".docx", ".doc"}
    PDF_EXTENSIONS = {".pdf"}

    def __init__(
        self,
        selector: DocumentSelector,
        extractor: ArchiveExtractor,
        downloader: Optional[DocumentDownloader] = None,
    ):
        self.selector = selector
        self.extractor = extractor
        self.downloader = downloader
        self._excel_preparator = ExcelPreparator()
        self._archive_processor = ArchiveProcessor(selector, extractor, downloader)

    def prepare_workbook_paths(
        self,
        records: List[Dict[str, Any]],
        documents: Optional[List[Dict[str, Any]]],
        tender_folder: Path,
    ) -> tuple[List[Path], List[Path], List[Path]]:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ø–∏—Å–∫–∏ –ø–æ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω–Ω—ã—Ö –ø—É—Ç–µ–π."""
        # #region agent log
        import json
        import os
        project_root = os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
        log_path = os.path.join(project_root, ".cursor", "debug.log")
        try:
            with open(log_path, "a", encoding="utf-8") as f:
                f.write(json.dumps({
                    "sessionId": "debug-session",
                    "runId": "doc-processing-debug",
                    "hypothesisId": "PREPARE_PATHS_START",
                    "location": "workbook_manager.py:prepare_workbook_paths:start",
                    "message": "–ù–∞—á–∏–Ω–∞–µ–º prepare_workbook_paths",
                    "data": {
                        "records_count": len(records),
                        "tender_folder": str(tender_folder),
                        "folder_exists": tender_folder.exists(),
                        "documents_count": len(documents) if documents else 0
                    },
                    "timestamp": int(time.time() * 1000)
                }))
        except Exception as e:
            pass
        # #endregion

        logger.info(f"üîç –ù–∞—á–∏–Ω–∞–µ–º –ø–æ–¥–≥–æ—Ç–æ–≤–∫—É –ø—É—Ç–µ–π –∫ —Ñ–∞–π–ª–∞–º: {len(records)} –∑–∞–ø–∏—Å–µ–π")

        workbook_paths_dict: Dict[Tuple[str, int], Path] = {}
        workbook_paths_set: Set[Path] = set()
        archive_paths: List[Path] = []
        queue: List[Dict[str, Any]] = [self._normalize_record(record) for record in records]
        duplicates_count = 0
        processed_files = 0

        logger.info(f"üì¶ –û—á–µ—Ä–µ–¥—å –æ–±—Ä–∞–±–æ—Ç–∫–∏: {len(queue)} –∑–∞–ø–∏—Å–µ–π")
        while queue:
            processed_files += 1
            if processed_files % 10 == 0:
                logger.debug(f"–û–±—Ä–∞–±–æ—Ç–∞–Ω–æ —Ñ–∞–π–ª–æ–≤: {processed_files}, –æ—Å—Ç–∞–ª–æ—Å—å –≤ –æ—á–µ—Ä–µ–¥–∏: {len(queue)}")
            record = queue.pop(0)

            # #region agent log
            try:
                with open(log_path, "a", encoding="utf-8") as f:
                    f.write(json.dumps({
                        "sessionId": "debug-session",
                        "runId": "doc-processing-debug",
                        "hypothesisId": "PROCESS_RECORD",
                        "location": "workbook_manager.py:prepare_workbook_paths:process_record",
                        "message": f"–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –∑–∞–ø–∏—Å—å —Å {len(record['paths'])} —Ñ–∞–π–ª–∞–º–∏",
                        "data": {
                            "record_paths": [str(p)[-50:] for p in record["paths"]],
                            "processed_files": processed_files,
                            "queue_remaining": len(queue)
                        },
                        "timestamp": int(time.time() * 1000)
                    }))
            except Exception:
                pass
            # #endregion

            for file_path in record["paths"]:
                path = Path(file_path).resolve()
                if not path.exists():
                    # #region agent log
                    try:
                        with open(log_path, "a", encoding="utf-8") as f:
                            f.write(json.dumps({
                                "sessionId": "debug-session",
                                "runId": "doc-processing-debug",
                                "hypothesisId": "FILE_MISSING",
                                "location": "workbook_manager.py:prepare_workbook_paths:file_missing",
                                "message": f"–§–∞–π–ª –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç: {path.name}",
                                "data": {"file_path": str(path)},
                                "timestamp": int(time.time() * 1000)
                            }))
                    except Exception:
                        pass
                    # #endregion
                    continue
                
                # –°–Ω–∞—á–∞–ª–∞ –æ–ø—Ä–µ–¥–µ–ª—è–µ–º, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ —Ñ–∞–π–ª –∞—Ä—Ö–∏–≤–æ–º (–ø–æ —Ä–∞—Å—à–∏—Ä–µ–Ω–∏—é –ò–õ–ò –ø–æ —Å–æ–¥–µ—Ä–∂–∏–º–æ–º—É)
                suffix = path.suffix.lower()
                is_archive = suffix in self.ARCHIVE_EXTENSIONS
                
                # –ï—Å–ª–∏ —Ä–∞—Å—à–∏—Ä–µ–Ω–∏–µ –Ω–µ –∞—Ä—Ö–∏–≤–Ω–æ–µ, –Ω–æ —Ñ–∞–π–ª –º–æ–∂–µ—Ç –±—ã—Ç—å –∞—Ä—Ö–∏–≤–æ–º - –ø—Ä–æ–≤–µ—Ä—è–µ–º —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ
                if not is_archive and self.extractor.is_file_archive(path):
                    logger.info(f"–§–∞–π–ª {path.name} –∏–º–µ–µ—Ç —Ä–∞—Å—à–∏—Ä–µ–Ω–∏–µ {suffix}, –Ω–æ —è–≤–ª—è–µ—Ç—Å—è –∞—Ä—Ö–∏–≤–æ–º. –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –∫–∞–∫ –∞—Ä—Ö–∏–≤.")
                    is_archive = True
                
                if is_archive:
                    # –≠—Ç–æ –∞—Ä—Ö–∏–≤ - —Ä–∞—Å–ø–∞–∫–æ–≤—ã–≤–∞–µ–º
                    logger.info(f"üì¶ –ù–∞—á–∏–Ω–∞–µ–º —Ä–∞—Å–ø–∞–∫–æ–≤–∫—É –∞—Ä—Ö–∏–≤–∞: {path.name} (—Ä–∞–∑–º–µ—Ä: {path.stat().st_size / 1024 / 1024:.2f} –ú–ë)")
                    archive_paths.append(path)

                    # #region agent log
                    try:
                        with open(log_path, "a", encoding="utf-8") as f:
                            f.write(json.dumps({
                                "sessionId": "debug-session",
                                "runId": "doc-processing-debug",
                                "hypothesisId": "BEFORE_PROCESS_ARCHIVE",
                                "location": "workbook_manager.py:prepare_workbook_paths:before_process_archive",
                                "message": f"–ü–µ—Ä–µ–¥ –≤—ã–∑–æ–≤–æ–º process_archive_path –¥–ª—è {path.name}",
                                "data": {
                                    "archive_path": str(path),
                                    "archive_size": path.stat().st_size,
                                    "tender_folder": str(tender_folder)
                                },
                                "timestamp": int(time.time() * 1000)
                            }))
                    except Exception:
                        pass
                    # #endregion

                    success = self._archive_processor.process_archive_path(
                        path,
                        record,
                        documents,
                        tender_folder,
                        queue,
                        workbook_paths_dict,
                        workbook_paths_set,
                    )
                    if not success:
                        logger.warning(f"‚ùå –ê—Ä—Ö–∏–≤ {path.name} –ø—Ä–æ–ø—É—â–µ–Ω –∏–∑-–∑–∞ –æ—à–∏–±–æ–∫")
                    else:
                        logger.info(f"‚úÖ –ê—Ä—Ö–∏–≤ {path.name} —É—Å–ø–µ—à–Ω–æ –æ–±—Ä–∞–±–æ—Ç–∞–Ω")
                elif suffix in self.EXCEL_EXTENSIONS:
                    # –≠—Ç–æ Excel —Ñ–∞–π–ª (–Ω–µ –∞—Ä—Ö–∏–≤) - –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –Ω–∞–ø—Ä—è–º—É—é
                    if path.name.startswith("~$"):
                        continue
                    prepared_path = self._excel_preparator.prepare_excel_file(path, tender_folder)
                    if prepared_path:
                        duplicates_count += add_file_to_dict(
                            prepared_path, workbook_paths_dict, workbook_paths_set, "Excel"
                        )
                    else:
                        # –§–∞–π–ª –ø–æ–≤—Ä–µ–∂–¥–µ–Ω - —É–¥–∞–ª—è–µ–º –∏—Å—Ö–æ–¥–Ω—ã–π —Ñ–∞–π–ª (–æ–Ω —É–∂–µ —É–¥–∞–ª–µ–Ω –≤ prepare_excel_file)
                        logger.warning(f"Excel —Ñ–∞–π–ª {path.name} –ø–æ–≤—Ä–µ–∂–¥–µ–Ω –∏ —É–¥–∞–ª–µ–Ω, –±—É–¥–µ—Ç –ø—Ä–æ–ø—É—â–µ–Ω")
                elif suffix in self.PDF_EXTENSIONS:
                    # PDF —Ñ–∞–π–ª—ã –¥–æ–±–∞–≤–ª—è–µ–º –Ω–∞–ø—Ä—è–º—É—é (–±–µ–∑ –∫–æ–ø–∏—Ä–æ–≤–∞–Ω–∏—è)
                    duplicates_count += add_file_to_dict(
                        path, workbook_paths_dict, workbook_paths_set, "PDF"
                    )
                elif suffix in self.WORD_EXTENSIONS:
                    # Word —Ñ–∞–π–ª—ã –¥–æ–±–∞–≤–ª—è–µ–º –Ω–∞–ø—Ä—è–º—É—é (–±–µ–∑ –∫–æ–ø–∏—Ä–æ–≤–∞–Ω–∏—è)
                    duplicates_count += add_file_to_dict(
                        path, workbook_paths_dict, workbook_paths_set, "Word"
                    )
                else:
                    logger.debug(f"–ü—Ä–æ–ø—É—Å–∫ –Ω–µ–ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º–æ–≥–æ —Ñ–∞–π–ª–∞ {path.name} (—Ä–∞—Å—à–∏—Ä–µ–Ω–∏–µ: {suffix})")

        workbook_paths = list(workbook_paths_dict.values())
        if duplicates_count > 0:
            logger.info(
                f"–î–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏—è —Ñ–∞–π–ª–æ–≤: –Ω–∞–π–¥–µ–Ω–æ {duplicates_count} –¥—É–±–ª–∏–∫–∞—Ç–æ–≤, "
                f"—É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö {len(workbook_paths)}"
            )
        logger.info(f"–ü–æ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω–æ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏: {len(workbook_paths)}")

        # #region agent log
        try:
            with open(log_path, "a", encoding="utf-8") as f:
                f.write(json.dumps({
                    "sessionId": "debug-session",
                    "runId": "doc-processing-debug",
                    "hypothesisId": "PREPARE_PATHS_COMPLETE",
                    "location": "workbook_manager.py:prepare_workbook_paths:complete",
                    "message": "prepare_workbook_paths –∑–∞–≤–µ—Ä—à–µ–Ω —É—Å–ø–µ—à–Ω–æ",
                    "data": {
                        "workbook_paths_count": len(workbook_paths),
                        "archive_paths_count": len(archive_paths),
                        "duplicates_removed": duplicates_count,
                        "sample_workbook_paths": [str(p)[-50:] for p in workbook_paths[:3]] if workbook_paths else []
                    },
                    "timestamp": int(time.time() * 1000)
                }))
        except Exception:
            pass
        # #endregion

        return workbook_paths, archive_paths, workbook_paths.copy()

        # except block intentionally removed because we now raise upstream

    @staticmethod
    def _normalize_record(record: Dict[str, Any]) -> Dict[str, Any]:
        """
        –ù–æ—Ä–º–∞–ª–∏–∑—É–µ—Ç –∑–∞–ø–∏—Å—å –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏.
        
        Args:
            record: –ó–∞–ø–∏—Å—å —Å –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º–∏
            
        Returns:
            –ù–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω–∞—è –∑–∞–ø–∏—Å—å
        """
        normalized = dict(record)
        normalized["paths"] = [Path(p) for p in record.get("paths", [])]
        normalized["retries"] = record.get("retries", 0)
        return normalized

