"""
MODULE: scripts.process_downloaded_archives
RESPONSIBILITY: Processing downloaded archives, extracting XLSX, and matching with the DB.
ALLOWED: sys, time, pathlib, typing, loguru, config.settings, core.database, core.exceptions, services.document_search_service, services.archive_processing_service.
FORBIDDEN: None.
ERRORS: None.

–ë–æ–µ–≤–æ–π —Å–∫—Ä–∏–ø—Ç –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ —É–∂–µ —Å–∫–∞—á–∞–Ω–Ω—ã—Ö –∞—Ä—Ö–∏–≤–æ–≤ –∏–∑ –ø–∞–ø–∫–∏ –∑–∞–≥—Ä—É–∑–∫–∏.

–ù–∞—Ö–æ–¥–∏—Ç –≤—Å–µ –∞—Ä—Ö–∏–≤—ã –≤ –ø–∞–ø–∫–µ, –æ–±—ä–µ–¥–∏–Ω—è–µ—Ç –º–Ω–æ–≥–æ—Ñ–∞–π–ª–æ–≤—ã–µ, —Ä–∞—Å–ø–∞–∫–æ–≤—ã–≤–∞–µ—Ç,
–∏—â–µ—Ç XLSX —Ñ–∞–π–ª—ã –∏ —Å–≤–µ—Ä—è–µ—Ç —Å –¥–∞–Ω–Ω—ã–º–∏ –∏–∑ –ë–î.
"""

from __future__ import annotations

import sys
import time
from pathlib import Path
from typing import List, Dict, Any

from loguru import logger

from config.settings import config
from core.database import DatabaseManager
from core.exceptions import DocumentSearchError, DatabaseConnectionError
from services.document_search_service import DocumentSearchService
from services.archive_processing_service import (
    ArchiveProcessingService,
    find_archives_in_directory,
)


def process_archive_group(
    processor: ArchiveProcessingService,
    base_name: str,
    archive_paths: List[Path],
    download_root: Path,
) -> Dict[str, Any]:
    """
    –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –æ–¥–Ω—É –≥—Ä—É–ø–ø—É –∞—Ä—Ö–∏–≤–æ–≤ (–º–Ω–æ–≥–æ—Ñ–∞–π–ª–æ–≤—ã–π –∏–ª–∏ –æ–¥–∏–Ω–æ—á–Ω—ã–π).
    
    Args:
        service: –°–µ—Ä–≤–∏—Å –ø–æ–∏—Å–∫–∞ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–∏
        base_name: –ë–∞–∑–æ–≤–æ–µ –∏–º—è –∞—Ä—Ö–∏–≤–∞
        archive_paths: –ü—É—Ç–∏ –∫ —á–∞—Å—Ç—è–º –∞—Ä—Ö–∏–≤–∞
        
    Returns:
        –†–µ–∑—É–ª—å—Ç–∞—Ç –æ–±—Ä–∞–±–æ—Ç–∫–∏: –ø—É—Ç—å –∫ XLSX –∏ –Ω–∞–π–¥–µ–Ω–Ω—ã–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è
    """
    logger.info(f"\n{'='*80}")
    logger.info(f"–û–±—Ä–∞–±–æ—Ç–∫–∞ –∞—Ä—Ö–∏–≤–∞: {base_name}")
    logger.info(f"–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —á–∞—Å—Ç–µ–π: {len(archive_paths)}")
    logger.info(f"{'='*80}")
    
    start_time = time.time()
    
    try:
        result = processor.process_archive_group(base_name, archive_paths)
    except DocumentSearchError as error:
        elapsed_time = time.time() - start_time
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ –∞—Ä—Ö–∏–≤–∞ {base_name}: {error}")
        return {
            "workbook_paths": [],
            "matches": [],
            "error": str(error),
            "processing_time": elapsed_time,
            "total_size": 0,
            "files_count": 0,
        }

    elapsed_time = time.time() - start_time
    workbook_paths = result.workbook_paths
    matches = result.matches
    total_size = result.total_size

    logger.info(f"\n‚úÖ –ê—Ä—Ö–∏–≤ —É—Å–ø–µ—à–Ω–æ –æ–±—Ä–∞–±–æ—Ç–∞–Ω!")
    if workbook_paths:
        logger.info("üìÑ –ù–∞–π–¥–µ–Ω–Ω—ã–µ XLSX —Ñ–∞–π–ª—ã:")
        for path in workbook_paths:
            size_mb = path.stat().st_size / (1024 * 1024) if path.exists() else 0
            logger.info(f"  - {path.name} ({size_mb:.2f} –ú–ë)")
    logger.info(f"üîç –ù–∞–π–¥–µ–Ω–æ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–π —Å –ë–î: {len(matches)}")
    logger.info(f"‚è±Ô∏è  –í—Ä–µ–º—è –æ–±—Ä–∞–±–æ—Ç–∫–∏: {elapsed_time:.2f} —Å–µ–∫")
    logger.info(f"üìä –†–∞–∑–º–µ—Ä –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤: {total_size / (1024 * 1024):.2f} –ú–ë")

    groups = processor.group_matches_by_score(matches)
    if groups["exact"] or groups["good"]:
        logger.info("\nüìã –ù–∞–π–¥–µ–Ω–Ω—ã–µ —Ç–æ–≤–∞—Ä—ã:")
        counter = 0

        def log_group(title: str, items: List[Dict[str, Any]]) -> None:
            nonlocal counter
            if not items:
                return
            logger.info(f"\n{title}")
            for match in items:
                counter += 1
                display = processor.build_display_chunks(match, download_root)
                logger.info(
                    f"  {counter}. {match['product_name']} "
                    f"(—Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ: {match['score']:.1f}%)"
                )
                logger.info(f"      {display['file_info']}")
                if display["summary"]:
                    logger.info(f"      {display['summary']}")
                logger.info(f"      {display['cell_text']}")

        log_group("‚úÖ –¢–û–ß–ù–´–ï –°–û–í–ü–ê–î–ï–ù–ò–Ø (100%)", groups["exact"])
        log_group("üîç –•–û–†–û–®–ò–ï –°–û–í–ü–ê–î–ï–ù–ò–Ø (85%+)", groups["good"])
    else:
        logger.warning("‚ö†Ô∏è  –°–æ–≤–ø–∞–¥–µ–Ω–∏–π —Å —Ç–æ–≤–∞—Ä–∞–º–∏ –∏–∑ –ë–î –Ω–µ –Ω–∞–π–¥–µ–Ω–æ")

    return {
        "workbook_paths": [str(path) for path in workbook_paths],
        "matches": matches,
        "processing_time": elapsed_time,
        "total_size": total_size,
        "files_count": len(workbook_paths),
    }


def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Å–∫–∞—á–∞–Ω–Ω—ã—Ö –∞—Ä—Ö–∏–≤–æ–≤."""
    
    # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è –≤ –∫–æ–Ω—Å–æ–ª—å
    logger.remove()
    logger.add(
        sys.stderr,
        level="INFO",
        format="{time:HH:mm:ss} | {level: <8} | {message}",
        colorize=True
    )
    
    logger.info("üöÄ –ó–∞–ø—É—Å–∫ –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Å–∫–∞—á–∞–Ω–Ω—ã—Ö –∞—Ä—Ö–∏–≤–æ–≤")
    logger.info("="*80)
    
    overall_start_time = time.time()
    
    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é –∑–∞–≥—Ä—É–∑–∫–∏
    if config.document_download_dir:
        download_dir = Path(config.document_download_dir).expanduser().resolve()
    else:
        logger.error("‚ùå DOCUMENT_DOWNLOAD_DIR –Ω–µ –Ω–∞—Å—Ç—Ä–æ–µ–Ω –≤ .env —Ñ–∞–π–ª–µ!")
        sys.exit(1)
    
    logger.info(f"üìÅ –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –∑–∞–≥—Ä—É–∑–∫–∏: {download_dir}")
    
    if not download_dir.exists():
        logger.error(f"‚ùå –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç: {download_dir}")
        sys.exit(1)
    
    # –ü–æ–¥–∫–ª—é—á–∞–µ–º—Å—è –∫ –ë–î
    try:
        db_manager = DatabaseManager(config.database)
        db_manager.connect()
        logger.info("‚úÖ –ü–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ –ë–î —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–æ")
    except DatabaseConnectionError as error:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è –∫ –ë–î: {error}")
        sys.exit(1)
    
    # –°–æ–∑–¥–∞–µ–º —Å–µ—Ä–≤–∏—Å—ã
    try:
        service = DocumentSearchService(
            db_manager,
            download_dir,
            unrar_path=config.unrar_tool,
            winrar_path=config.winrar_path,
        )
        processor = ArchiveProcessingService(service)
        logger.info("‚úÖ –°–µ—Ä–≤–∏—Å –ø–æ–∏—Å–∫–∞ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–∏ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω")
    except Exception as error:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ —Å–µ—Ä–≤–∏—Å–∞: {error}")
        db_manager.close()
        sys.exit(1)
    
    # –ù–∞—Ö–æ–¥–∏–º –≤—Å–µ –∞—Ä—Ö–∏–≤—ã
    archive_groups = find_archives_in_directory(download_dir)
    
    if not archive_groups:
        logger.warning("‚ö†Ô∏è  –ê—Ä—Ö–∏–≤—ã –Ω–µ –Ω–∞–π–¥–µ–Ω—ã –≤ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏")
        service.db_manager.close()
        return
    
    # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –∫–∞–∂–¥—É—é –≥—Ä—É–ø–ø—É –∞—Ä—Ö–∏–≤–æ–≤
    results = []
    for base_name, archive_paths in archive_groups.items():
        result = process_archive_group(processor, base_name, archive_paths, download_dir)
        results.append({
            "archive_name": base_name,
            "parts_count": len(archive_paths),
            **result
        })
    
    # –ò—Ç–æ–≥–æ–≤–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
    overall_elapsed_time = time.time() - overall_start_time
    
    logger.info(f"\n{'='*80}")
    logger.info("üìä –ò–¢–û–ì–û–í–ê–Ø –°–¢–ê–¢–ò–°–¢–ò–ö–ê")
    logger.info(f"{'='*80}")
    
    total_archives = len(results)
    successful = sum(1 for r in results if r.get("workbook_paths"))
    total_matches = sum(len(r.get("matches", [])) for r in results)
    
    # –°–æ–±–∏—Ä–∞–µ–º –æ–±—â—É—é —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –ø–æ —Ñ–∞–π–ª–∞–º
    total_files_processed = sum(r.get("files_count", 0) for r in results)
    total_size_bytes = sum(r.get("total_size", 0) for r in results)
    total_processing_time = sum(r.get("processing_time", 0) for r in results)
    
    logger.info(f"üì¶ –í—Å–µ–≥–æ –∞—Ä—Ö–∏–≤–æ–≤ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ: {total_archives}")
    logger.info(f"‚úÖ –£—Å–ø–µ—à–Ω–æ —Ä–∞—Å–ø–∞–∫–æ–≤–∞–Ω–æ: {successful}")
    logger.info(f"üìÑ –ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–æ —Ñ–∞–π–ª–æ–≤: {total_files_processed}")
    logger.info(f"üíæ –û–±—â–∏–π —Ä–∞–∑–º–µ—Ä —Ñ–∞–π–ª–æ–≤: {total_size_bytes / (1024 * 1024):.2f} –ú–ë")
    logger.info(f"‚è±Ô∏è  –û–±—â–µ–µ –≤—Ä–µ–º—è –æ–±—Ä–∞–±–æ—Ç–∫–∏: {overall_elapsed_time:.2f} —Å–µ–∫")
    logger.info(f"‚è±Ô∏è  –í—Ä–µ–º—è –Ω–∞ –ø–æ–∏—Å–∫ (—Å—É–º–º–∞—Ä–Ω–æ): {total_processing_time:.2f} —Å–µ–∫")
    logger.info(f"üîç –í—Å–µ–≥–æ –Ω–∞–π–¥–µ–Ω–æ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–π: {total_matches}")
    
    if total_files_processed > 0:
        avg_time_per_file = total_processing_time / total_files_processed
        logger.info(f"‚ö° –°—Ä–µ–¥–Ω–µ–µ –≤—Ä–µ–º—è –Ω–∞ —Ñ–∞–π–ª: {avg_time_per_file:.2f} —Å–µ–∫")
    
    # –ó–∞–∫—Ä—ã–≤–∞–µ–º —Å–æ–µ–¥–∏–Ω–µ–Ω–∏–µ —Å –ë–î
    service.db_manager.close()
    logger.info("\n‚úÖ –û–±—Ä–∞–±–æ—Ç–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞")


if __name__ == "__main__":
    main()

